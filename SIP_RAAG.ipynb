{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22f2f91-9c9b-420c-9a01-eebb35507fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> TRAINING SETTING: Alphabet\n",
      "Epoch 000 | Loss: 0.4128\n",
      "Epoch 001 | Loss: 0.2606\n",
      "Epoch 002 | Loss: 0.2221\n",
      "Epoch 003 | Loss: 0.1819\n",
      "Epoch 004 | Loss: 0.1385\n",
      "Epoch 005 | Loss: 0.0987\n",
      "Epoch 006 | Loss: 0.0754\n",
      "Epoch 007 | Loss: 0.0528\n",
      "Epoch 008 | Loss: 0.0491\n",
      "Epoch 009 | Loss: 0.0430\n",
      "Epoch 010 | Loss: 0.0359\n",
      "Epoch 011 | Loss: 0.0322\n",
      "Epoch 012 | Loss: 0.0293\n",
      "Epoch 013 | Loss: 0.0255\n",
      "Epoch 014 | Loss: 0.0195\n",
      "Epoch 015 | Loss: 0.0167\n",
      "Epoch 016 | Loss: 0.0116\n",
      "Epoch 017 | Loss: 0.0129\n",
      "Epoch 018 | Loss: 0.0143\n",
      "Epoch 019 | Loss: 0.0119\n",
      "Epoch 020 | Loss: 0.0093\n",
      "Testing Alphabet...\n",
      "\n",
      ">>> TRAINING SETTING: Identity\n",
      "Epoch 000 | Loss: 0.3013\n",
      "Epoch 001 | Loss: 0.0008\n",
      "Testing Identity...\n",
      "\n",
      ">>> TRAINING SETTING: Commutator\n",
      "Epoch 000 | Loss: 0.5417\n",
      "Epoch 001 | Loss: 0.3774\n",
      "Epoch 002 | Loss: 0.3033\n",
      "Epoch 003 | Loss: 0.2716\n",
      "Epoch 004 | Loss: 0.2126\n",
      "Epoch 005 | Loss: 0.1577\n",
      "Epoch 006 | Loss: 0.1222\n",
      "Epoch 007 | Loss: 0.1010\n",
      "Epoch 008 | Loss: 0.0831\n",
      "Epoch 009 | Loss: 0.0592\n",
      "Epoch 010 | Loss: 0.0561\n",
      "Epoch 011 | Loss: 0.0500\n",
      "Epoch 012 | Loss: 0.0496\n",
      "Epoch 013 | Loss: 0.0404\n",
      "Epoch 014 | Loss: 0.0222\n",
      "Epoch 015 | Loss: 0.0272\n",
      "Epoch 016 | Loss: 0.0195\n",
      "Epoch 017 | Loss: 0.0205\n",
      "Epoch 018 | Loss: 0.0308\n",
      "Epoch 019 | Loss: 0.0190\n",
      "Epoch 020 | Loss: 0.0213\n",
      "Epoch 021 | Loss: 0.0256\n",
      "Epoch 022 | Loss: 0.0220\n",
      "Epoch 023 | Loss: 0.0334\n",
      "Epoch 024 | Loss: 0.0151\n",
      "Epoch 025 | Loss: 0.0183\n",
      "Epoch 026 | Loss: 0.0197\n",
      "Epoch 027 | Loss: 0.0199\n",
      "Epoch 028 | Loss: 0.0176\n",
      "Epoch 029 | Loss: 0.0088\n",
      "Testing Commutator...\n",
      "\n",
      ">>> TRAINING SETTING: Swap\n",
      "Epoch 000 | Loss: 0.5836\n",
      "Epoch 001 | Loss: 0.4798\n",
      "Epoch 002 | Loss: 0.3043\n",
      "Epoch 003 | Loss: 0.2029\n",
      "Epoch 004 | Loss: 0.2088\n",
      "Epoch 005 | Loss: 0.1823\n",
      "Epoch 006 | Loss: 0.1693\n",
      "Epoch 007 | Loss: 0.1635\n",
      "Epoch 008 | Loss: 0.1641\n",
      "Epoch 009 | Loss: 0.1759\n",
      "Epoch 010 | Loss: 0.1611\n",
      "Epoch 011 | Loss: 0.1635\n",
      "Epoch 012 | Loss: 0.1678\n",
      "Epoch 013 | Loss: 0.1609\n",
      "Epoch 014 | Loss: 0.1541\n",
      "Epoch 015 | Loss: 0.1722\n",
      "Epoch 016 | Loss: 0.1633\n",
      "Epoch 017 | Loss: 0.1593\n",
      "Epoch 018 | Loss: 0.1458\n",
      "Epoch 019 | Loss: 0.1547\n",
      "Epoch 020 | Loss: 0.1469\n",
      "Epoch 021 | Loss: 0.1644\n",
      "Epoch 022 | Loss: 0.1643\n",
      "Epoch 023 | Loss: 0.1507\n",
      "Epoch 024 | Loss: 0.1674\n",
      "Epoch 025 | Loss: 0.1556\n",
      "Epoch 026 | Loss: 0.1585\n",
      "Epoch 027 | Loss: 0.1488\n",
      "Epoch 028 | Loss: 0.1445\n",
      "Epoch 029 | Loss: 0.1497\n",
      "Epoch 030 | Loss: 0.1579\n",
      "Epoch 031 | Loss: 0.1493\n",
      "Epoch 032 | Loss: 0.1547\n",
      "Epoch 033 | Loss: 0.1417\n",
      "Epoch 034 | Loss: 0.1578\n",
      "Epoch 035 | Loss: 0.1523\n",
      "Epoch 036 | Loss: 0.1592\n",
      "Epoch 037 | Loss: 0.1390\n",
      "Epoch 038 | Loss: 0.1458\n",
      "Epoch 039 | Loss: 0.1441\n",
      "Epoch 040 | Loss: 0.1465\n",
      "Epoch 041 | Loss: 0.1497\n",
      "Epoch 042 | Loss: 0.1378\n",
      "Epoch 043 | Loss: 0.1552\n",
      "Epoch 044 | Loss: 0.1621\n",
      "Epoch 045 | Loss: 0.1504\n",
      "Epoch 046 | Loss: 0.1479\n",
      "Epoch 047 | Loss: 0.1386\n",
      "Epoch 048 | Loss: 0.1497\n",
      "Epoch 049 | Loss: 0.1340\n",
      "Epoch 050 | Loss: 0.1459\n",
      "Epoch 051 | Loss: 0.1408\n",
      "Epoch 052 | Loss: 0.1447\n",
      "Epoch 053 | Loss: 0.1475\n",
      "Epoch 054 | Loss: 0.1592\n",
      "Epoch 055 | Loss: 0.1451\n",
      "Epoch 056 | Loss: 0.1309\n",
      "Epoch 057 | Loss: 0.1507\n",
      "Epoch 058 | Loss: 0.1504\n",
      "Epoch 059 | Loss: 0.1439\n",
      "Epoch 060 | Loss: 0.1467\n",
      "Epoch 061 | Loss: 0.1474\n",
      "Epoch 062 | Loss: 0.1350\n",
      "Epoch 063 | Loss: 0.1345\n",
      "Epoch 064 | Loss: 0.1287\n",
      "Epoch 065 | Loss: 0.1384\n",
      "Epoch 066 | Loss: 0.1435\n",
      "Epoch 067 | Loss: 0.1358\n",
      "Epoch 068 | Loss: 0.1383\n",
      "Epoch 069 | Loss: 0.1377\n",
      "Epoch 070 | Loss: 0.1420\n",
      "Epoch 071 | Loss: 0.1411\n",
      "Epoch 072 | Loss: 0.1366\n",
      "Epoch 073 | Loss: 0.1452\n",
      "Epoch 074 | Loss: 0.1425\n",
      "Epoch 075 | Loss: 0.1417\n",
      "Epoch 076 | Loss: 0.1390\n",
      "Epoch 077 | Loss: 0.1364\n",
      "Epoch 078 | Loss: 0.1434\n",
      "Epoch 079 | Loss: 0.1389\n",
      "Epoch 080 | Loss: 0.1340\n",
      "Epoch 081 | Loss: 0.1450\n",
      "Epoch 082 | Loss: 0.1344\n",
      "Epoch 083 | Loss: 0.1292\n",
      "Epoch 084 | Loss: 0.1412\n",
      "Epoch 085 | Loss: 0.1380\n",
      "Epoch 086 | Loss: 0.1444\n",
      "Epoch 087 | Loss: 0.1392\n",
      "Epoch 088 | Loss: 0.1181\n",
      "Epoch 089 | Loss: 0.1358\n",
      "Epoch 090 | Loss: 0.1528\n",
      "Epoch 091 | Loss: 0.1309\n",
      "Epoch 092 | Loss: 0.1339\n",
      "Epoch 093 | Loss: 0.1362\n",
      "Epoch 094 | Loss: 0.1490\n",
      "Epoch 095 | Loss: 0.1414\n",
      "Epoch 096 | Loss: 0.1461\n",
      "Epoch 097 | Loss: 0.1419\n",
      "Epoch 098 | Loss: 0.1480\n",
      "Epoch 099 | Loss: 0.1439\n",
      "Epoch 100 | Loss: 0.1343\n",
      "Epoch 101 | Loss: 0.1408\n",
      "Epoch 102 | Loss: 0.1297\n",
      "Epoch 103 | Loss: 0.1460\n",
      "Epoch 104 | Loss: 0.1271\n",
      "Epoch 105 | Loss: 0.1189\n",
      "Epoch 106 | Loss: 0.1311\n",
      "Epoch 107 | Loss: 0.1236\n",
      "Epoch 108 | Loss: 0.1278\n",
      "Epoch 109 | Loss: 0.1351\n",
      "Epoch 110 | Loss: 0.1294\n",
      "Epoch 111 | Loss: 0.1265\n",
      "Epoch 112 | Loss: 0.1363\n",
      "Epoch 113 | Loss: 0.1116\n",
      "Epoch 114 | Loss: 0.1389\n",
      "Epoch 115 | Loss: 0.1415\n",
      "Epoch 116 | Loss: 0.1446\n",
      "Epoch 117 | Loss: 0.1318\n",
      "Epoch 118 | Loss: 0.1372\n",
      "Epoch 119 | Loss: 0.1216\n",
      "Epoch 120 | Loss: 0.1137\n",
      "Epoch 121 | Loss: 0.1246\n",
      "Epoch 122 | Loss: 0.1118\n",
      "Epoch 123 | Loss: 0.1154\n",
      "Epoch 124 | Loss: 0.1181\n",
      "Epoch 125 | Loss: 0.1111\n",
      "Epoch 126 | Loss: 0.1187\n",
      "Epoch 127 | Loss: 0.1181\n",
      "Epoch 128 | Loss: 0.1164\n",
      "Epoch 129 | Loss: 0.1209\n",
      "Epoch 130 | Loss: 0.1477\n",
      "Epoch 131 | Loss: 0.1428\n",
      "Epoch 132 | Loss: 0.1323\n",
      "Epoch 133 | Loss: 0.1471\n",
      "Epoch 134 | Loss: 0.1366\n",
      "Epoch 135 | Loss: 0.1251\n",
      "Epoch 136 | Loss: 0.1040\n",
      "Epoch 137 | Loss: 0.1271\n",
      "Epoch 138 | Loss: 0.1188\n",
      "Epoch 139 | Loss: 0.1140\n",
      "Epoch 140 | Loss: 0.0961\n",
      "Epoch 141 | Loss: 0.1026\n",
      "Epoch 142 | Loss: 0.0963\n",
      "Epoch 143 | Loss: 0.1536\n",
      "Epoch 144 | Loss: 0.1327\n",
      "Epoch 145 | Loss: 0.1174\n",
      "Epoch 146 | Loss: 0.0974\n",
      "Epoch 147 | Loss: 0.0929\n",
      "Epoch 148 | Loss: 0.0917\n",
      "Epoch 149 | Loss: 0.1869\n",
      "Epoch 150 | Loss: 0.1449\n",
      "Epoch 151 | Loss: 0.1371\n",
      "Epoch 152 | Loss: 0.1548\n",
      "Epoch 153 | Loss: 0.1347\n",
      "Epoch 154 | Loss: 0.1434\n",
      "Epoch 155 | Loss: 0.1215\n",
      "Epoch 156 | Loss: 0.1394\n",
      "Epoch 157 | Loss: 0.1185\n",
      "Epoch 158 | Loss: 0.0990\n",
      "Epoch 159 | Loss: 0.0966\n",
      "Epoch 160 | Loss: 0.0982\n",
      "Epoch 161 | Loss: 0.0975\n",
      "Epoch 162 | Loss: 0.0930\n",
      "Epoch 163 | Loss: 0.0987\n",
      "Epoch 164 | Loss: 0.0868\n",
      "Epoch 165 | Loss: 0.0788\n",
      "Epoch 166 | Loss: 0.0878\n",
      "Epoch 167 | Loss: 0.0995\n",
      "Epoch 168 | Loss: 0.1243\n",
      "Epoch 169 | Loss: 0.1259\n",
      "Epoch 170 | Loss: 0.0827\n",
      "Epoch 171 | Loss: 0.0933\n",
      "Epoch 172 | Loss: 0.1100\n",
      "Epoch 173 | Loss: 0.0817\n",
      "Epoch 174 | Loss: 0.0833\n",
      "Epoch 175 | Loss: 0.0860\n",
      "Epoch 176 | Loss: 0.0754\n",
      "Epoch 177 | Loss: 0.0675\n",
      "Epoch 178 | Loss: 0.0795\n",
      "Epoch 179 | Loss: 0.0961\n",
      "Epoch 180 | Loss: 0.0833\n",
      "Epoch 181 | Loss: 0.0793\n",
      "Epoch 182 | Loss: 0.0994\n",
      "Epoch 183 | Loss: 0.0964\n",
      "Epoch 184 | Loss: 0.1137\n",
      "Epoch 185 | Loss: 0.0822\n",
      "Epoch 186 | Loss: 0.0730\n",
      "Epoch 187 | Loss: 0.0678\n",
      "Epoch 188 | Loss: 0.0665\n",
      "Epoch 189 | Loss: 0.0712\n",
      "Epoch 190 | Loss: 0.0699\n",
      "Epoch 191 | Loss: 0.0867\n",
      "Epoch 192 | Loss: 0.0520\n",
      "Epoch 193 | Loss: 0.0683\n",
      "Epoch 194 | Loss: 0.1025\n",
      "Epoch 195 | Loss: 0.0933\n",
      "Epoch 196 | Loss: 0.0591\n",
      "Epoch 197 | Loss: 0.0711\n",
      "Epoch 198 | Loss: 0.0865\n",
      "Epoch 199 | Loss: 0.0599\n",
      "Epoch 200 | Loss: 0.0804\n",
      "Epoch 201 | Loss: 0.0695\n",
      "Epoch 202 | Loss: 0.0820\n",
      "Epoch 203 | Loss: 0.0673\n",
      "Epoch 204 | Loss: 0.0745\n",
      "Epoch 205 | Loss: 0.0724\n",
      "Epoch 206 | Loss: 0.0587\n",
      "Epoch 207 | Loss: 0.0659\n",
      "Epoch 208 | Loss: 0.0598\n",
      "Epoch 209 | Loss: 0.0660\n",
      "Epoch 210 | Loss: 0.0589\n",
      "Epoch 211 | Loss: 0.0527\n",
      "Epoch 212 | Loss: 0.0678\n",
      "Epoch 213 | Loss: 0.0677\n",
      "Epoch 214 | Loss: 0.0726\n",
      "Epoch 215 | Loss: 0.0683\n",
      "Epoch 216 | Loss: 0.0829\n",
      "Epoch 217 | Loss: 0.0688\n",
      "Epoch 218 | Loss: 0.1377\n",
      "Epoch 219 | Loss: 0.0984\n",
      "Epoch 220 | Loss: 0.0712\n",
      "Epoch 221 | Loss: 0.0522\n",
      "Epoch 222 | Loss: 0.0576\n",
      "Epoch 223 | Loss: 0.0534\n",
      "Epoch 224 | Loss: 0.0613\n",
      "Epoch 225 | Loss: 0.0595\n",
      "Epoch 226 | Loss: 0.0570\n",
      "Epoch 227 | Loss: 0.0498\n",
      "Epoch 228 | Loss: 0.0480\n",
      "Epoch 229 | Loss: 0.0592\n",
      "Epoch 230 | Loss: 0.0617\n",
      "Epoch 231 | Loss: 0.0455\n",
      "Epoch 232 | Loss: 0.0399\n",
      "Epoch 233 | Loss: 0.0459\n",
      "Epoch 234 | Loss: 0.0650\n",
      "Epoch 235 | Loss: 0.0486\n",
      "Epoch 236 | Loss: 0.0584\n",
      "Epoch 237 | Loss: 0.0687\n",
      "Epoch 238 | Loss: 0.0472\n",
      "Epoch 239 | Loss: 0.0491\n",
      "Epoch 240 | Loss: 0.0464\n",
      "Epoch 241 | Loss: 0.0499\n",
      "Epoch 242 | Loss: 0.0512\n",
      "Epoch 243 | Loss: 0.0433\n",
      "Epoch 244 | Loss: 0.0708\n",
      "Epoch 245 | Loss: 0.1376\n",
      "Epoch 246 | Loss: 0.1329\n",
      "Epoch 247 | Loss: 0.1302\n",
      "Epoch 248 | Loss: 0.1143\n",
      "Epoch 249 | Loss: 0.0881\n",
      "Epoch 250 | Loss: 0.0859\n",
      "Epoch 251 | Loss: 0.0678\n",
      "Epoch 252 | Loss: 0.0862\n",
      "Epoch 253 | Loss: 0.1712\n",
      "Epoch 254 | Loss: 0.1132\n",
      "Epoch 255 | Loss: 0.0676\n",
      "Epoch 256 | Loss: 0.0449\n",
      "Epoch 257 | Loss: 0.0525\n",
      "Epoch 258 | Loss: 0.0370\n",
      "Epoch 259 | Loss: 0.0681\n",
      "Epoch 260 | Loss: 0.0703\n",
      "Epoch 261 | Loss: 0.0376\n",
      "Epoch 262 | Loss: 0.0632\n",
      "Epoch 263 | Loss: 0.0582\n",
      "Epoch 264 | Loss: 0.0521\n",
      "Epoch 265 | Loss: 0.0509\n",
      "Epoch 266 | Loss: 0.0368\n",
      "Epoch 267 | Loss: 0.0398\n",
      "Epoch 268 | Loss: 0.0451\n",
      "Epoch 269 | Loss: 0.0578\n",
      "Epoch 270 | Loss: 0.0413\n",
      "Epoch 271 | Loss: 0.0442\n",
      "Epoch 272 | Loss: 0.0334\n",
      "Epoch 273 | Loss: 0.0392\n",
      "Epoch 274 | Loss: 0.0367\n",
      "Epoch 275 | Loss: 0.0372\n",
      "Epoch 276 | Loss: 0.0393\n",
      "Epoch 277 | Loss: 0.0366\n",
      "Epoch 278 | Loss: 0.0975\n",
      "Epoch 279 | Loss: 0.0475\n",
      "Epoch 280 | Loss: 0.0489\n",
      "Epoch 281 | Loss: 0.0509\n",
      "Epoch 282 | Loss: 0.0447\n",
      "Epoch 283 | Loss: 0.0477\n",
      "Epoch 284 | Loss: 0.0336\n",
      "Epoch 285 | Loss: 0.0336\n",
      "Epoch 286 | Loss: 0.0532\n",
      "Epoch 287 | Loss: 0.0507\n",
      "Epoch 288 | Loss: 0.0371\n",
      "Epoch 289 | Loss: 0.0357\n",
      "Epoch 290 | Loss: 0.0284\n",
      "Epoch 291 | Loss: 0.0361\n",
      "Epoch 292 | Loss: 0.0389\n",
      "Epoch 293 | Loss: 0.0357\n",
      "Epoch 294 | Loss: 0.0436\n",
      "Epoch 295 | Loss: 0.0874\n",
      "Epoch 296 | Loss: 0.0399\n",
      "Epoch 297 | Loss: 0.0386\n",
      "Epoch 298 | Loss: 0.0315\n",
      "Epoch 299 | Loss: 0.0328\n",
      "Epoch 300 | Loss: 0.0356\n",
      "Epoch 301 | Loss: 0.0281\n",
      "Epoch 302 | Loss: 0.0269\n",
      "Epoch 303 | Loss: 0.0564\n",
      "Epoch 304 | Loss: 0.0329\n",
      "Epoch 305 | Loss: 0.0518\n",
      "Epoch 306 | Loss: 0.0309\n",
      "Epoch 307 | Loss: 0.0310\n",
      "Epoch 308 | Loss: 0.0333\n",
      "Epoch 309 | Loss: 0.0283\n",
      "Epoch 310 | Loss: 0.0392\n",
      "Epoch 311 | Loss: 0.0204\n",
      "Epoch 312 | Loss: 0.0282\n",
      "Epoch 313 | Loss: 0.0252\n",
      "Epoch 314 | Loss: 0.0367\n",
      "Epoch 315 | Loss: 0.0380\n",
      "Epoch 316 | Loss: 0.0356\n",
      "Epoch 317 | Loss: 0.0315\n",
      "Epoch 318 | Loss: 0.0299\n",
      "Epoch 319 | Loss: 0.0281\n",
      "Epoch 320 | Loss: 0.0535\n",
      "Epoch 321 | Loss: 0.0357\n",
      "Epoch 322 | Loss: 0.0348\n",
      "Epoch 323 | Loss: 0.0260\n",
      "Epoch 324 | Loss: 0.0290\n",
      "Epoch 325 | Loss: 0.0397\n",
      "Epoch 326 | Loss: 0.0243\n",
      "Epoch 327 | Loss: 0.0209\n",
      "Epoch 328 | Loss: 0.0321\n",
      "Epoch 329 | Loss: 0.0241\n",
      "Epoch 330 | Loss: 0.0257\n",
      "Epoch 331 | Loss: 0.0330\n",
      "Epoch 332 | Loss: 0.0222\n",
      "Epoch 333 | Loss: 0.0207\n",
      "Epoch 334 | Loss: 0.0344\n",
      "Epoch 335 | Loss: 0.0327\n",
      "Epoch 336 | Loss: 0.0287\n",
      "Epoch 337 | Loss: 0.0329\n",
      "Epoch 338 | Loss: 0.0160\n",
      "Epoch 339 | Loss: 0.0262\n",
      "Epoch 340 | Loss: 0.0343\n",
      "Epoch 341 | Loss: 0.0212\n",
      "Epoch 342 | Loss: 0.0212\n",
      "Epoch 343 | Loss: 0.0258\n",
      "Epoch 344 | Loss: 0.0397\n",
      "Epoch 345 | Loss: 0.0298\n",
      "Epoch 346 | Loss: 0.0438\n",
      "Epoch 347 | Loss: 0.0344\n",
      "Epoch 348 | Loss: 0.0223\n",
      "Epoch 349 | Loss: 0.0142\n",
      "Epoch 350 | Loss: 0.0276\n",
      "Epoch 351 | Loss: 0.0176\n",
      "Epoch 352 | Loss: 0.0268\n",
      "Epoch 353 | Loss: 0.0352\n",
      "Epoch 354 | Loss: 0.0300\n",
      "Epoch 355 | Loss: 0.0255\n",
      "Epoch 356 | Loss: 0.0238\n",
      "Epoch 357 | Loss: 0.0221\n",
      "Epoch 358 | Loss: 0.0266\n",
      "Epoch 359 | Loss: 0.0234\n",
      "Epoch 360 | Loss: 0.0255\n",
      "Epoch 361 | Loss: 0.0164\n",
      "Epoch 362 | Loss: 0.0381\n",
      "Epoch 363 | Loss: 0.0359\n",
      "Epoch 364 | Loss: 0.0220\n",
      "Epoch 365 | Loss: 0.0256\n",
      "Epoch 366 | Loss: 0.0163\n",
      "Epoch 367 | Loss: 0.0218\n",
      "Epoch 368 | Loss: 0.0471\n",
      "Epoch 369 | Loss: 0.0247\n",
      "Epoch 370 | Loss: 0.0291\n",
      "Epoch 371 | Loss: 0.0264\n",
      "Epoch 372 | Loss: 0.0236\n",
      "Epoch 373 | Loss: 0.0201\n",
      "Epoch 374 | Loss: 0.0175\n",
      "Epoch 375 | Loss: 0.0061\n",
      "Testing Swap...\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON REPORT 10\n",
      "==================================================\n",
      "Alphabet     | Scrambled: 0.7027 | Neg: -0.1462 | GAP: 0.8489\n",
      "Identity     | Scrambled: 0.9735 | Neg: 0.3069 | GAP: 0.6666\n",
      "Commutator   | Scrambled: 0.9279 | Neg: 0.1371 | GAP: 0.7908\n",
      "Swap         | Scrambled: 0.9841 | Neg: -0.4590 | GAP: 1.4431\n",
      "\n",
      ">>> TRAINING SETTING: Alphabet\n",
      "Epoch 000 | Loss: 0.0333\n",
      "Epoch 001 | Loss: 0.0294\n",
      "Epoch 002 | Loss: 0.0272\n",
      "Epoch 003 | Loss: 0.0246\n",
      "Epoch 004 | Loss: 0.0200\n",
      "Epoch 005 | Loss: 0.0172\n",
      "Epoch 006 | Loss: 0.0104\n",
      "Epoch 007 | Loss: 0.0114\n",
      "Epoch 008 | Loss: 0.0126\n",
      "Epoch 009 | Loss: 0.0072\n",
      "Testing Alphabet...\n",
      "\n",
      ">>> TRAINING SETTING: Identity\n",
      "Epoch 000 | Loss: 0.0003\n",
      "Testing Identity...\n",
      "\n",
      ">>> TRAINING SETTING: Commutator\n",
      "Epoch 000 | Loss: 0.0022\n",
      "Testing Commutator...\n",
      "\n",
      ">>> TRAINING SETTING: Swap\n",
      "Epoch 000 | Loss: 0.0296\n",
      "Epoch 001 | Loss: 0.0198\n",
      "Epoch 002 | Loss: 0.0239\n",
      "Epoch 003 | Loss: 0.0197\n",
      "Epoch 004 | Loss: 0.0144\n",
      "Epoch 005 | Loss: 0.0168\n",
      "Epoch 006 | Loss: 0.0163\n",
      "Epoch 007 | Loss: 0.0279\n",
      "Epoch 008 | Loss: 0.0250\n",
      "Epoch 009 | Loss: 0.0319\n",
      "Epoch 010 | Loss: 0.0217\n",
      "Epoch 011 | Loss: 0.0450\n",
      "Epoch 012 | Loss: 0.1160\n",
      "Epoch 013 | Loss: 0.0512\n",
      "Epoch 014 | Loss: 0.0525\n",
      "Epoch 015 | Loss: 0.0262\n",
      "Epoch 016 | Loss: 0.0285\n",
      "Epoch 017 | Loss: 0.0262\n",
      "Epoch 018 | Loss: 0.0182\n",
      "Epoch 019 | Loss: 0.0232\n",
      "Epoch 020 | Loss: 0.0229\n",
      "Epoch 021 | Loss: 0.0198\n",
      "Epoch 022 | Loss: 0.0226\n",
      "Epoch 023 | Loss: 0.0207\n",
      "Epoch 024 | Loss: 0.0193\n",
      "Epoch 025 | Loss: 0.0277\n",
      "Epoch 026 | Loss: 0.0209\n",
      "Epoch 027 | Loss: 0.0179\n",
      "Epoch 028 | Loss: 0.0216\n",
      "Epoch 029 | Loss: 0.0161\n",
      "Epoch 030 | Loss: 0.0153\n",
      "Epoch 031 | Loss: 0.0170\n",
      "Epoch 032 | Loss: 0.0197\n",
      "Epoch 033 | Loss: 0.0148\n",
      "Epoch 034 | Loss: 0.0172\n",
      "Epoch 035 | Loss: 0.0131\n",
      "Epoch 036 | Loss: 0.0246\n",
      "Epoch 037 | Loss: 0.0178\n",
      "Epoch 038 | Loss: 0.0137\n",
      "Epoch 039 | Loss: 0.0152\n",
      "Epoch 040 | Loss: 0.0164\n",
      "Epoch 041 | Loss: 0.0287\n",
      "Epoch 042 | Loss: 0.0340\n",
      "Epoch 043 | Loss: 0.0219\n",
      "Epoch 044 | Loss: 0.0146\n",
      "Epoch 045 | Loss: 0.0177\n",
      "Epoch 046 | Loss: 0.0202\n",
      "Epoch 047 | Loss: 0.0351\n",
      "Epoch 048 | Loss: 0.0176\n",
      "Epoch 049 | Loss: 0.0199\n",
      "Epoch 050 | Loss: 0.0197\n",
      "Epoch 051 | Loss: 0.0413\n",
      "Epoch 052 | Loss: 0.0248\n",
      "Epoch 053 | Loss: 0.0118\n",
      "Epoch 054 | Loss: 0.0184\n",
      "Epoch 055 | Loss: 0.0168\n",
      "Epoch 056 | Loss: 0.0146\n",
      "Epoch 057 | Loss: 0.0155\n",
      "Epoch 058 | Loss: 0.0151\n",
      "Epoch 059 | Loss: 0.0218\n",
      "Epoch 060 | Loss: 0.0126\n",
      "Epoch 061 | Loss: 0.0325\n",
      "Epoch 062 | Loss: 0.0355\n",
      "Epoch 063 | Loss: 0.0184\n",
      "Epoch 064 | Loss: 0.0227\n",
      "Epoch 065 | Loss: 0.0186\n",
      "Epoch 066 | Loss: 0.0228\n",
      "Epoch 067 | Loss: 0.0166\n",
      "Epoch 068 | Loss: 0.0213\n",
      "Epoch 069 | Loss: 0.1024\n",
      "Epoch 070 | Loss: 0.1267\n",
      "Epoch 071 | Loss: 0.0413\n",
      "Epoch 072 | Loss: 0.0196\n",
      "Epoch 073 | Loss: 0.0218\n",
      "Epoch 074 | Loss: 0.0174\n",
      "Epoch 075 | Loss: 0.0214\n",
      "Epoch 076 | Loss: 0.0173\n",
      "Epoch 077 | Loss: 0.0147\n",
      "Epoch 078 | Loss: 0.0267\n",
      "Epoch 079 | Loss: 0.0212\n",
      "Epoch 080 | Loss: 0.0129\n",
      "Epoch 081 | Loss: 0.0145\n",
      "Epoch 082 | Loss: 0.0199\n",
      "Epoch 083 | Loss: 0.0187\n",
      "Epoch 084 | Loss: 0.0126\n",
      "Epoch 085 | Loss: 0.0090\n",
      "Testing Swap...\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON REPORT 20\n",
      "==================================================\n",
      "Alphabet     | Scrambled: 0.6290 | Neg: -0.2219 | GAP: 0.8510\n",
      "Identity     | Scrambled: 0.9737 | Neg: 0.3696 | GAP: 0.6042\n",
      "Commutator   | Scrambled: 0.9818 | Neg: 0.2152 | GAP: 0.7666\n",
      "Swap         | Scrambled: 0.9904 | Neg: -0.1513 | GAP: 1.1417\n",
      "\n",
      ">>> TRAINING SETTING: Alphabet\n",
      "Epoch 000 | Loss: 0.0277\n",
      "Epoch 001 | Loss: 0.0184\n",
      "Epoch 002 | Loss: 0.0128\n",
      "Epoch 003 | Loss: 0.0196\n",
      "Epoch 004 | Loss: 0.0107\n",
      "Epoch 005 | Loss: 0.0097\n",
      "Testing Alphabet...\n",
      "\n",
      ">>> TRAINING SETTING: Identity\n",
      "Epoch 000 | Loss: 0.0001\n",
      "Testing Identity...\n",
      "\n",
      ">>> TRAINING SETTING: Commutator\n",
      "Epoch 000 | Loss: 0.0000\n",
      "Testing Commutator...\n",
      "\n",
      ">>> TRAINING SETTING: Swap\n",
      "Epoch 000 | Loss: 0.0080\n",
      "Testing Swap...\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON REPORT 30\n",
      "==================================================\n",
      "Alphabet     | Scrambled: 0.5977 | Neg: -0.2115 | GAP: 0.8092\n",
      "Identity     | Scrambled: 0.9703 | Neg: 0.2221 | GAP: 0.7482\n",
      "Commutator   | Scrambled: 0.9888 | Neg: 0.2270 | GAP: 0.7618\n",
      "Swap         | Scrambled: 0.9895 | Neg: -0.0102 | GAP: 0.9997\n",
      "\n",
      ">>> TRAINING SETTING: Alphabet\n",
      "Epoch 000 | Loss: 0.0179\n",
      "Epoch 001 | Loss: 0.0106\n",
      "Epoch 002 | Loss: 0.0131\n",
      "Epoch 003 | Loss: 0.0106\n",
      "Epoch 004 | Loss: 0.0075\n",
      "Testing Alphabet...\n",
      "\n",
      ">>> TRAINING SETTING: Identity\n",
      "Epoch 000 | Loss: 0.0000\n",
      "Testing Identity...\n",
      "\n",
      ">>> TRAINING SETTING: Commutator\n",
      "Epoch 000 | Loss: 0.0000\n",
      "Testing Commutator...\n",
      "\n",
      ">>> TRAINING SETTING: Swap\n",
      "Epoch 000 | Loss: 0.0169\n",
      "Epoch 001 | Loss: 0.0092\n",
      "Testing Swap...\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON REPORT 40\n",
      "==================================================\n",
      "Alphabet     | Scrambled: 0.5083 | Neg: -0.3278 | GAP: 0.8361\n",
      "Identity     | Scrambled: 0.9479 | Neg: 0.2521 | GAP: 0.6958\n",
      "Commutator   | Scrambled: 0.9895 | Neg: 0.1925 | GAP: 0.7970\n",
      "Swap         | Scrambled: 0.9886 | Neg: -0.2907 | GAP: 1.2793\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "# --- 1. RAAG DEFINITION & REDUCTION ---\n",
    "# Commutation Graph: Even nodes commute with the next odd node\n",
    "RAAG_ADJ = torch.zeros((2000, 2000))\n",
    "for i in range(1999):\n",
    "    if i % 2 == 0:\n",
    "        RAAG_ADJ[i][i+1] = 1\n",
    "        RAAG_ADJ[i+1][i] = 1\n",
    "\n",
    "class RAAGWord:\n",
    "    def __init__(self, letters, adj_matrix=RAAG_ADJ):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.letters = self.reduce(letters)\n",
    "\n",
    "    def reduce(self, lts):\n",
    "        res = []\n",
    "        for l in lts:\n",
    "            if not res:\n",
    "                res.append(l)\n",
    "                continue\n",
    "\n",
    "            # RAAG Reduction: Shuffle past commuting elements to find inverse\n",
    "            idx = len(res) - 1\n",
    "            can_cancel = False\n",
    "            while idx >= 0:\n",
    "                if res[idx] == -l:\n",
    "                    can_cancel = True\n",
    "                    break\n",
    "                # If 'l' doesn't commute with res[idx], it's blocked from moving further\n",
    "                if not self.adj_matrix[abs(l)][abs(res[idx])]:\n",
    "                    break\n",
    "                idx -= 1\n",
    "\n",
    "            if can_cancel:\n",
    "                res.pop(idx)\n",
    "            else:\n",
    "                res.append(l)\n",
    "        return res\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return RAAGWord(self.letters + other.letters, self.adj_matrix)\n",
    "\n",
    "    def inv(self):\n",
    "        return RAAGWord([-l for l in reversed(self.letters)], self.adj_matrix)\n",
    "\n",
    "# --- 2. ALGEBRAIC MOVES & TOPOLOGY ---\n",
    "def nielsen_move(gens):\n",
    "    new_gens = list(gens)\n",
    "    i, j = random.sample(range(len(gens)), 2)\n",
    "    op = random.choice(['mul', 'inv_mul'])\n",
    "    if op == 'mul':\n",
    "        new_gens[i] = (new_gens[i][0] * new_gens[j][0], new_gens[i][1] * new_gens[j][1])\n",
    "    else:\n",
    "        j_inv = (new_gens[j][0].inv(), new_gens[j][1].inv())\n",
    "        new_gens[i] = (new_gens[i][0] * j_inv[0], new_gens[i][1] * j_inv[1])\n",
    "    return new_gens\n",
    "\n",
    "def subgroup_to_graph(subgroup):\n",
    "    x, edge_index = [], []\n",
    "    # 5 Distinct Hubs (using 501-505 as identifiers)\n",
    "    for i in range(5): x.append([float(501 + i)])\n",
    "    curr_idx = 5\n",
    "    for i, (w_a, w_b) in enumerate(subgroup):\n",
    "        hub_idx = i\n",
    "        full_word = w_a.letters + w_b.letters\n",
    "        if not full_word: full_word = [0]\n",
    "        for j, letter in enumerate(full_word):\n",
    "            x.append([float(letter)])\n",
    "            # Connect letter node to the respective generator hub\n",
    "            edge_index.append([curr_idx, hub_idx]); edge_index.append([hub_idx, curr_idx])\n",
    "            # Sequence edges (path topology)\n",
    "            if j > 0:\n",
    "                edge_index.append([curr_idx - 1, curr_idx]); edge_index.append([curr_idx, curr_idx - 1])\n",
    "            curr_idx += 1\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous())\n",
    "\n",
    "# --- 3. MODEL: UNIVERSAL HUB GNN ---\n",
    "class UniversalHubGNN(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.embed = nn.Embedding(2001, hidden) # Handle range -1000 to 1000\n",
    "        self.lin_deg = nn.Linear(1, hidden)\n",
    "        self.convs = nn.ModuleList([\n",
    "            GINConv(nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, hidden)))\n",
    "            for _ in range(6)\n",
    "        ])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden) for _ in range(6)])\n",
    "        # Input to FC is 5 hubs * hidden_size\n",
    "        self.fc = nn.Linear(5 * hidden, 64)\n",
    "\n",
    "        # Add this method to your UniversalHubGNN class\n",
    "        def forward(self, x, edge_index, batch=None, **kwargs):\n",
    "            from torch_geometric.data import Data\n",
    "            # Reconstruct the object your forward_one expects\n",
    "            data = Data(x=x, edge_index=edge_index, batch=batch)\n",
    "            return self.forward_one(data)\n",
    "\n",
    "    def forward_one(self, data):\n",
    "        # Normalizing input indices for embedding\n",
    "        idx = torch.clamp((data.x.view(-1) + 1000).long(), 0, 2000)\n",
    "        x = self.embed(idx)\n",
    "\n",
    "        # Degree-based structural features\n",
    "        row, _ = data.edge_index\n",
    "        deg = torch.zeros((data.x.size(0), 1), device=data.x.device)\n",
    "        deg.scatter_add_(0, row.unsqueeze(1), torch.ones((row.size(0), 1), device=data.x.device))\n",
    "        x = x + self.lin_deg(deg)\n",
    "\n",
    "        for conv, ln in zip(self.convs, self.lns):\n",
    "            h = conv(x, data.edge_index)\n",
    "            x = ln(F.dropout(F.relu(h), p=0.1, training=self.training) + x)\n",
    "\n",
    "        # Robust Slot-Aware Pooling\n",
    "        num_graphs = data.num_graphs if hasattr(data, 'num_graphs') else 1\n",
    "        hub_embeddings = []\n",
    "        if hasattr(data, 'ptr') and data.ptr is not None:\n",
    "            for i in range(num_graphs):\n",
    "                start = data.ptr[i]\n",
    "                hub_embeddings.append(x[start : start + 5].reshape(-1))\n",
    "        else:\n",
    "            hub_embeddings.append(x[:5].reshape(-1))\n",
    "\n",
    "        x_final = torch.stack(hub_embeddings)\n",
    "        return F.normalize(self.fc(x_final), p=2, dim=1)\n",
    "\n",
    "# --- 4. DATA GENERATION: UPDATED ---\n",
    "def generate_triplets(num_samples, complexity, neg_type):\n",
    "    triplets = []\n",
    "    for _ in range(num_samples):\n",
    "        shift = random.randint(-400, 400)\n",
    "        A = [RAAGWord([shift + i]) for i in range(1, 6)]\n",
    "        B = [RAAGWord([shift + i + 100]) for i in range(1, 6)]\n",
    "        anchor_sub = [(A[i], B[i]) for i in range(5)]\n",
    "\n",
    "        # Positive: Scrambled via Nielsen moves\n",
    "        pos_sub = anchor_sub\n",
    "        for _ in range(complexity):\n",
    "            pos_sub = nielsen_move(pos_sub)\n",
    "\n",
    "        # Negative Logic\n",
    "        if neg_type == \"Alphabet\":\n",
    "            ns = shift + random.choice([-300, 300])\n",
    "            neg_sub = [(RAAGWord([ns+i]), RAAGWord([ns+i+100])) for i in range(1, 6)]\n",
    "        elif neg_type == \"Identity\":\n",
    "            neg_sub = list(anchor_sub)\n",
    "            neg_sub[random.randint(0, 4)] = (RAAGWord([]), RAAGWord([]))\n",
    "        elif neg_type == \"Commutator\":\n",
    "            neg_sub = list(anchor_sub)\n",
    "            i, j = random.sample(range(5), 2)\n",
    "            g1, g2 = anchor_sub[i], anchor_sub[j]\n",
    "            # Replace a generator with its commutator\n",
    "            neg_sub[random.randint(0, 4)] = (\n",
    "                g1[0]*g2[0]*g1[0].inv()*g2[0].inv(),\n",
    "                g1[1]*g2[1]*g1[1].inv()*g2[1].inv()\n",
    "            )\n",
    "        else: # \"Swap\" - Surgical change in one slot\n",
    "            neg_sub = list(anchor_sub)\n",
    "            idx = random.randint(0, 4)\n",
    "            neg_sub[idx] = (RAAGWord([shift + 999]), anchor_sub[idx][1])\n",
    "\n",
    "        triplets.append((\n",
    "            subgroup_to_graph(anchor_sub),\n",
    "            subgroup_to_graph(pos_sub),\n",
    "            subgroup_to_graph(neg_sub)\n",
    "        ))\n",
    "    return triplets\n",
    "\n",
    "def collate(batch):\n",
    "    return (Batch.from_data_list([b[0] for b in batch]),\n",
    "            Batch.from_data_list([b[1] for b in batch]),\n",
    "            Batch.from_data_list([b[2] for b in batch]))\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neg_cases = [\"Alphabet\", \"Identity\", \"Commutator\", \"Swap\"]\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "models = {}\n",
    "models = {case: None for case in neg_cases}\n",
    "\n",
    "for nielsen_moves in range(10, 41, 10):\n",
    "  final_results = {}\n",
    "  for case in neg_cases:\n",
    "      print(f\"\\n>>> TRAINING SETTING: {case}\")\n",
    "      if models[case] == None:\n",
    "          model = UniversalHubGNN().to(device)\n",
    "      else:\n",
    "          model = models[case]\n",
    "      optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002)\n",
    "      criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "\n",
    "      for epoch in range(500): # Adjust epoch count as needed\n",
    "          train_loader = DataLoader(generate_triplets(512, nielsen_moves, case), batch_size=16, collate_fn=collate, shuffle=True)\n",
    "          model.train()\n",
    "          t_loss = 0\n",
    "          for a, p, n in train_loader:\n",
    "              optimizer.zero_grad()\n",
    "              ea, ep, en = model.forward_one(a.to(device)), model.forward_one(p.to(device)), model.forward_one(n.to(device))\n",
    "              loss = criterion(ea, ep, en)\n",
    "              loss.backward(); optimizer.step(); t_loss += loss.item()\n",
    "\n",
    "\n",
    "          print(f\"Epoch {epoch:03d} | Loss: {t_loss/len(train_loader):.4f}\")\n",
    "          if t_loss/len(train_loader) < 0.01:\n",
    "            break\n",
    "\n",
    "      # Stress Test\n",
    "      print(f\"Testing {case}...\")\n",
    "      model.eval()\n",
    "      test_data = generate_triplets(100, nielsen_moves, case)\n",
    "      p_sims, n_sims = [], []\n",
    "      with torch.no_grad():\n",
    "          for a, p, n in test_data:\n",
    "              ea, ep, en = model.forward_one(a.to(device)), model.forward_one(p.to(device)), model.forward_one(n.to(device))\n",
    "              p_sims.append(F.cosine_similarity(ea, ep).item())\n",
    "              n_sims.append(F.cosine_similarity(ea, en).item())\n",
    "\n",
    "      final_results[case] = {\"Pos\": np.mean(p_sims), \"Neg\": np.mean(n_sims)}\n",
    "      models[case] = model\n",
    "  all_results.append(final_results)\n",
    "  torch.save(model, 'full_gin_model.pth')\n",
    "\n",
    "  # --- FINAL REPORT ---\n",
    "  print(\"\\n\" + \"=\"*50 + f\"\\nFINAL COMPARISON REPORT {nielsen_moves}\\n\" + \"=\"*50)\n",
    "  for case, res in final_results.items():\n",
    "      print(f\"{case:<12} | Scrambled: {res['Pos']:.4f} | Neg: {res['Neg']:.4f} | GAP: {res['Pos']-res['Neg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6458f5b3-17d1-4c44-8052-8f6de0d578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”’ Hiding secret in Vault Slot: 76...\n",
      "\n",
      "ðŸ”Ž Testing Specialist: [Alphabet]\n",
      "   Result: âŒ FAILURE | Sim: 0.7938 | Found at: 52\n",
      "\n",
      "ðŸ”Ž Testing Specialist: [Identity]\n",
      "   Result: âŒ FAILURE | Sim: 0.9817 | Found at: 29\n",
      "\n",
      "ðŸ”Ž Testing Specialist: [Commutator]\n",
      "   Result: âŒ FAILURE | Sim: 0.9923 | Found at: 5\n",
      "\n",
      "ðŸ”Ž Testing Specialist: [Swap]\n",
      "   Result: âŒ FAILURE | Sim: 0.9875 | Found at: 93\n",
      "\n",
      "==================================================\n",
      "COMMITTEE CRACK SUMMARY\n",
      "==================================================\n",
      "Alphabet     : FAIL  | Confidence Gap: 0.4442\n",
      "Identity     : FAIL  | Confidence Gap: 0.0117\n",
      "Commutator   : FAIL  | Confidence Gap: 0.4612\n",
      "Swap         : FAIL  | Confidence Gap: 0.0048\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def apply_nielsen_moves_fixed(subgroup_basis, num_moves=50):\n",
    "    \"\"\"Scrambles the subgroup while maintaining your RAAGWord reduction logic.\"\"\"\n",
    "    scrambled = copy.deepcopy(subgroup_basis)\n",
    "    rank = len(scrambled)\n",
    "\n",
    "    for _ in range(num_moves):\n",
    "        i, j = random.sample(range(rank), 2)\n",
    "        op = random.choice(['mul', 'inv_mul'])\n",
    "        \n",
    "        # We use your existing __mul__ and inv() methods\n",
    "        if op == 'mul':\n",
    "            scrambled[i] = (scrambled[i][0] * scrambled[j][0], \n",
    "                            scrambled[i][1] * scrambled[j][1])\n",
    "        else:\n",
    "            j_inv = (scrambled[j][0].inv(), scrambled[j][1].inv())\n",
    "            scrambled[i] = (scrambled[i][0] * j_inv[0], \n",
    "                            scrambled[i][1] * j_inv[1])\n",
    "    return scrambled\n",
    "\n",
    "def execute_committee_crack(models_dict, device, secret_shift=42):\n",
    "    \"\"\"\n",
    "    Runs the crack test across all trained specialist models.\n",
    "    models_dict: { 'Alphabet': model1, 'Identity': model2, ... }\n",
    "    \"\"\"\n",
    "    # 1. Prepare the Secret Basis and Anchor Graph once\n",
    "    secret_basis = [(RAAGWord([secret_shift + i]), RAAGWord([secret_shift + i + 100])) for i in range(5)]\n",
    "    # Use the same vault for every model for a fair comparison\n",
    "    correct_index = random.randint(0, 99)\n",
    "    vault = []\n",
    "    \n",
    "    print(f\"ðŸ”’ Hiding secret in Vault Slot: {correct_index}...\")\n",
    "    for i in range(100):\n",
    "        if i == correct_index:\n",
    "            vault.append(apply_nielsen_moves_fixed(secret_basis, num_moves=40))\n",
    "        else:\n",
    "            r_shift = random.randint(-400, 400)\n",
    "            if abs(r_shift - secret_shift) < 50: r_shift += 200\n",
    "            noise_basis = [(RAAGWord([r_shift + j]), RAAGWord([r_shift + j + 100])) for j in range(5)]\n",
    "            vault.append(apply_nielsen_moves_fixed(noise_basis, num_moves=10))\n",
    "\n",
    "    # 2. Iterate through each specialist model\n",
    "    results_summary = {}\n",
    "\n",
    "    for case_name, model in models_dict.items():\n",
    "        if model is None: continue\n",
    "        \n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        print(f\"\\nðŸ”Ž Testing Specialist: [{case_name}]\")\n",
    "        \n",
    "        # Calculate Anchor Embedding for this specific model's latent space\n",
    "        anchor_graph = subgroup_to_graph(secret_basis).to(device)\n",
    "        with torch.no_grad():\n",
    "            anchor_emb = model.forward_one(anchor_graph)\n",
    "\n",
    "        similarities = []\n",
    "        for sub in vault:\n",
    "            g = subgroup_to_graph(sub).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = model.forward_one(g)\n",
    "                sim = F.cosine_similarity(anchor_emb, emb).item()\n",
    "                similarities.append(sim)\n",
    "\n",
    "        best_idx = np.argmax(similarities)\n",
    "        best_sim = similarities[best_idx]\n",
    "        success = (best_idx == correct_index)\n",
    "        \n",
    "        results_summary[case_name] = {\n",
    "            \"success\": success,\n",
    "            \"best_sim\": best_sim,\n",
    "            \"gap\": best_sim - np.mean(similarities) # How much the secret stands out\n",
    "        }\n",
    "        \n",
    "        status = \"âœ… SUCCESS\" if success else \"âŒ FAILURE\"\n",
    "        print(f\"   Result: {status} | Sim: {best_sim:.4f} | Found at: {best_idx}\")\n",
    "\n",
    "    # 3. Final Report\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\nCOMMITTEE CRACK SUMMARY\\n\" + \"=\"*50)\n",
    "    for case, data in results_summary.items():\n",
    "        outcome = \"PASS\" if data['success'] else \"FAIL\"\n",
    "        print(f\"{case:<12} : {outcome:<5} | Confidence Gap: {data['gap']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Run the updated crack\n",
    "execute_committee_crack(models, device, secret_shift=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce0079-d927-4e83-8809-7eb92bfada2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
