{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d22f2f91-9c9b-420c-9a01-eebb35507fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Initial Buffer...\n",
      "Epoch 0 | Avg Loss: 1.0731\n",
      "Epoch 5 | Avg Loss: 0.9376\n",
      "Epoch 10 | Avg Loss: 0.8802\n",
      "Epoch 15 | Avg Loss: 0.8065\n",
      "Epoch 20 | Avg Loss: 0.7299\n",
      "Epoch 25 | Avg Loss: 0.5655\n",
      "Epoch 30 | Avg Loss: 0.4641\n",
      "Epoch 35 | Avg Loss: 0.4156\n",
      "Epoch 40 | Avg Loss: 0.3521\n",
      "Epoch 45 | Avg Loss: 0.3070\n",
      "Epoch 50 | Avg Loss: 0.2988\n",
      "Epoch 55 | Avg Loss: 0.2998\n",
      "Epoch 60 | Avg Loss: 0.2783\n",
      "Epoch 65 | Avg Loss: 0.2225\n",
      "Epoch 70 | Avg Loss: 0.2015\n",
      "Epoch 75 | Avg Loss: 0.1863\n",
      "Epoch 80 | Avg Loss: 0.1630\n",
      "Epoch 85 | Avg Loss: 0.1340\n",
      "Epoch 90 | Avg Loss: 0.1578\n",
      "Epoch 95 | Avg Loss: 0.1348\n",
      "Epoch 100 | Avg Loss: 0.1234\n",
      "Epoch 105 | Avg Loss: 0.0942\n",
      "Epoch 110 | Avg Loss: 0.0866\n",
      "Epoch 115 | Avg Loss: 0.1001\n",
      "Epoch 120 | Avg Loss: 0.1005\n",
      "Epoch 125 | Avg Loss: 0.0716\n",
      "Epoch 130 | Avg Loss: 0.0768\n",
      "Epoch 135 | Avg Loss: 0.0872\n",
      "Epoch 140 | Avg Loss: 0.0921\n",
      "Epoch 145 | Avg Loss: 0.0954\n",
      "Epoch 150 | Avg Loss: 0.0649\n",
      "Epoch 155 | Avg Loss: 0.0667\n",
      "Epoch 160 | Avg Loss: 0.0683\n",
      "Epoch 165 | Avg Loss: 0.0521\n",
      "Epoch 170 | Avg Loss: 0.0749\n",
      "Epoch 175 | Avg Loss: 0.0505\n",
      "Epoch 180 | Avg Loss: 0.0601\n",
      "Epoch 185 | Avg Loss: 0.0744\n",
      "Epoch 190 | Avg Loss: 0.0514\n",
      "Epoch 195 | Avg Loss: 0.0589\n",
      "Epoch 200 | Avg Loss: 0.0688\n",
      "Epoch 205 | Avg Loss: 0.0475\n",
      "Epoch 210 | Avg Loss: 0.0418\n",
      "Epoch 215 | Avg Loss: 0.0744\n",
      "Epoch 220 | Avg Loss: 0.0417\n",
      "Epoch 225 | Avg Loss: 0.0928\n",
      "Epoch 230 | Avg Loss: 0.0670\n",
      "Epoch 235 | Avg Loss: 0.0496\n",
      "Epoch 240 | Avg Loss: 0.0402\n",
      "Epoch 245 | Avg Loss: 0.0502\n",
      "Epoch 250 | Avg Loss: 0.0273\n",
      "Epoch 255 | Avg Loss: 0.0483\n",
      "Epoch 260 | Avg Loss: 0.0404\n",
      "Epoch 265 | Avg Loss: 0.0464\n",
      "Epoch 270 | Avg Loss: 0.0596\n",
      "Epoch 275 | Avg Loss: 0.0504\n",
      "Epoch 280 | Avg Loss: 0.0509\n",
      "Epoch 285 | Avg Loss: 0.0373\n",
      "Epoch 290 | Avg Loss: 0.0295\n",
      "Epoch 295 | Avg Loss: 0.0995\n",
      "Epoch 300 | Avg Loss: 0.0438\n",
      "Epoch 305 | Avg Loss: 0.0444\n",
      "Epoch 310 | Avg Loss: 0.0275\n",
      "Epoch 315 | Avg Loss: 0.0243\n",
      "Epoch 320 | Avg Loss: 0.0224\n",
      "Epoch 325 | Avg Loss: 0.0337\n",
      "Epoch 330 | Avg Loss: 0.0243\n",
      "Epoch 335 | Avg Loss: 0.0241\n",
      "Epoch 340 | Avg Loss: 0.0412\n",
      "Epoch 345 | Avg Loss: 0.0478\n",
      "Epoch 350 | Avg Loss: 0.0532\n",
      "Epoch 355 | Avg Loss: 0.0222\n",
      "Epoch 360 | Avg Loss: 0.0259\n",
      "Epoch 365 | Avg Loss: 0.0390\n",
      "Epoch 370 | Avg Loss: 0.0731\n",
      "Epoch 375 | Avg Loss: 0.0503\n",
      "Epoch 380 | Avg Loss: 0.0392\n",
      "Epoch 385 | Avg Loss: 0.0251\n",
      "Epoch 390 | Avg Loss: 0.0202\n",
      "Epoch 395 | Avg Loss: 0.0438\n",
      "Epoch 400 | Avg Loss: 0.0229\n",
      "Epoch 405 | Avg Loss: 0.1025\n",
      "Epoch 410 | Avg Loss: 0.0681\n",
      "Epoch 415 | Avg Loss: 0.0451\n",
      "Epoch 420 | Avg Loss: 0.0419\n",
      "Epoch 425 | Avg Loss: 0.0310\n",
      "Epoch 430 | Avg Loss: 0.0511\n",
      "Epoch 435 | Avg Loss: 0.0538\n",
      "Epoch 440 | Avg Loss: 0.0305\n",
      "Epoch 445 | Avg Loss: 0.0344\n",
      "Epoch 450 | Avg Loss: 0.0683\n",
      "Epoch 455 | Avg Loss: 0.0733\n",
      "Epoch 460 | Avg Loss: 0.0338\n",
      "Epoch 465 | Avg Loss: 0.0362\n",
      "Epoch 470 | Avg Loss: 0.0234\n",
      "Epoch 475 | Avg Loss: 0.0265\n",
      "Epoch 480 | Avg Loss: 0.0179\n",
      "Epoch 485 | Avg Loss: 0.0224\n",
      "Epoch 490 | Avg Loss: 0.0254\n",
      "Epoch 495 | Avg Loss: 0.0224\n",
      "Epoch 500 | Avg Loss: 0.0204\n",
      "Epoch 505 | Avg Loss: 0.0333\n",
      "Epoch 510 | Avg Loss: 0.0206\n",
      "Epoch 515 | Avg Loss: 0.0155\n",
      "Epoch 520 | Avg Loss: 0.0213\n",
      "Epoch 525 | Avg Loss: 0.0201\n",
      "Epoch 530 | Avg Loss: 0.0224\n",
      "Epoch 535 | Avg Loss: 0.0223\n",
      "Epoch 540 | Avg Loss: 0.0251\n",
      "Epoch 545 | Avg Loss: 0.0142\n",
      "Epoch 550 | Avg Loss: 0.0151\n",
      "Epoch 555 | Avg Loss: 0.0278\n",
      "Epoch 560 | Avg Loss: 0.0207\n",
      "Epoch 565 | Avg Loss: 0.0237\n",
      "Epoch 570 | Avg Loss: 0.0175\n",
      "Epoch 575 | Avg Loss: 0.0216\n",
      "Epoch 580 | Avg Loss: 0.0309\n",
      "Epoch 585 | Avg Loss: 0.0148\n",
      "Epoch 590 | Avg Loss: 0.0152\n",
      "Epoch 595 | Avg Loss: 0.0179\n",
      "Epoch 600 | Avg Loss: 0.0125\n",
      "Epoch 605 | Avg Loss: 0.0116\n",
      "Epoch 610 | Avg Loss: 0.0860\n",
      "Epoch 615 | Avg Loss: 0.0238\n",
      "Epoch 620 | Avg Loss: 0.0266\n",
      "Epoch 625 | Avg Loss: 0.0382\n",
      "Epoch 630 | Avg Loss: 0.0211\n",
      "Epoch 635 | Avg Loss: 0.0139\n",
      "Epoch 640 | Avg Loss: 0.0125\n",
      "Epoch 645 | Avg Loss: 0.0114\n",
      "Epoch 650 | Avg Loss: 0.0154\n",
      "Epoch 655 | Avg Loss: 0.0171\n",
      "Epoch 660 | Avg Loss: 0.0298\n",
      "Epoch 665 | Avg Loss: 0.0274\n",
      "Epoch 670 | Avg Loss: 0.0132\n",
      "Epoch 675 | Avg Loss: 0.0240\n",
      "Epoch 680 | Avg Loss: 0.1361\n",
      "Epoch 685 | Avg Loss: 0.0290\n",
      "Epoch 690 | Avg Loss: 0.0216\n",
      "Epoch 695 | Avg Loss: 0.0280\n",
      "Epoch 700 | Avg Loss: 0.0261\n",
      "Epoch 705 | Avg Loss: 0.0194\n",
      "Epoch 710 | Avg Loss: 0.0155\n",
      "Epoch 715 | Avg Loss: 0.0448\n",
      "Epoch 720 | Avg Loss: 0.0160\n",
      "Epoch 725 | Avg Loss: 0.0106\n",
      "Epoch 730 | Avg Loss: 0.0285\n",
      "Epoch 735 | Avg Loss: 0.0158\n",
      "Epoch 740 | Avg Loss: 0.0148\n",
      "Epoch 745 | Avg Loss: 0.0434\n",
      "Epoch 750 | Avg Loss: 0.0136\n",
      "Epoch 755 | Avg Loss: 0.0094\n",
      "Epoch 760 | Avg Loss: 0.0130\n",
      "Epoch 765 | Avg Loss: 0.0088\n",
      "Epoch 770 | Avg Loss: 0.0125\n",
      "Epoch 775 | Avg Loss: 0.0148\n",
      "Epoch 780 | Avg Loss: 0.0117\n",
      "Epoch 785 | Avg Loss: 0.0086\n",
      "Epoch 790 | Avg Loss: 0.0198\n",
      "Epoch 795 | Avg Loss: 0.0084\n",
      "Epoch 800 | Avg Loss: 0.0074\n",
      "Epoch 805 | Avg Loss: 0.0059\n",
      "Epoch 810 | Avg Loss: 0.0246\n",
      "Epoch 815 | Avg Loss: 0.0337\n",
      "Epoch 820 | Avg Loss: 0.0357\n",
      "Epoch 825 | Avg Loss: 0.0238\n",
      "Epoch 830 | Avg Loss: 0.0274\n",
      "Epoch 835 | Avg Loss: 0.0216\n",
      "Epoch 840 | Avg Loss: 0.0150\n",
      "Epoch 845 | Avg Loss: 0.0173\n",
      "Epoch 850 | Avg Loss: 0.0139\n",
      "Epoch 855 | Avg Loss: 0.0093\n",
      "Epoch 860 | Avg Loss: 0.0139\n",
      "Epoch 865 | Avg Loss: 0.0115\n",
      "Epoch 870 | Avg Loss: 0.0173\n",
      "Epoch 875 | Avg Loss: 0.0114\n",
      "Epoch 880 | Avg Loss: 0.0085\n",
      "Epoch 885 | Avg Loss: 0.0084\n",
      "Epoch 890 | Avg Loss: 0.0115\n",
      "Epoch 895 | Avg Loss: 0.0064\n",
      "Epoch 900 | Avg Loss: 0.0157\n",
      "Epoch 905 | Avg Loss: 0.0095\n",
      "Epoch 910 | Avg Loss: 0.0188\n",
      "Epoch 915 | Avg Loss: 0.0237\n",
      "Epoch 920 | Avg Loss: 0.0153\n",
      "Epoch 925 | Avg Loss: 0.0231\n",
      "Epoch 930 | Avg Loss: 0.0108\n",
      "Epoch 935 | Avg Loss: 0.0072\n",
      "Epoch 940 | Avg Loss: 0.0204\n",
      "Epoch 945 | Avg Loss: 0.0099\n",
      "Epoch 950 | Avg Loss: 0.0075\n",
      "Epoch 955 | Avg Loss: 0.0086\n",
      "Epoch 960 | Avg Loss: 0.0084\n",
      "Epoch 965 | Avg Loss: 0.0075\n",
      "Epoch 970 | Avg Loss: 0.0080\n",
      "Epoch 975 | Avg Loss: 0.0087\n",
      "Epoch 980 | Avg Loss: 0.0081\n",
      "Epoch 985 | Avg Loss: 0.0068\n",
      "Epoch 990 | Avg Loss: 0.0069\n",
      "Epoch 995 | Avg Loss: 0.0119\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import gc\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# The specific imports for Graph Neural Networks\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "# --- 1. RAAG DEFINITION ---\n",
    "# Commutation Graph: Even nodes commute with the next odd node\n",
    "RAAG_ADJ = np.zeros((2001, 2001))\n",
    "for i in range(2000):\n",
    "    if i % 2 == 0:\n",
    "        RAAG_ADJ[i][i+1] = 1\n",
    "        RAAG_ADJ[i+1][i] = 1\n",
    "\n",
    "class RAAGWord:\n",
    "    def __init__(self, letters, adj_matrix=RAAG_ADJ):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.letters = self.reduce(letters)\n",
    "\n",
    "    def reduce(self, lts):\n",
    "        # SAFETY CAP: Prevent \"Infinite Growth\" memory explosion\n",
    "        if len(lts) > 60: lts = lts[:60]\n",
    "        \n",
    "        res = []\n",
    "        for l in lts:\n",
    "            if not res:\n",
    "                res.append(l)\n",
    "                continue\n",
    "            idx = len(res) - 1\n",
    "            can_cancel = False\n",
    "            while idx >= 0:\n",
    "                if res[idx] == -l:\n",
    "                    can_cancel = True\n",
    "                    break\n",
    "                if not self.adj_matrix[abs(l)][abs(res[idx])]:\n",
    "                    break\n",
    "                idx -= 1\n",
    "            if can_cancel:\n",
    "                res.pop(idx)\n",
    "            else:\n",
    "                res.append(l)\n",
    "        return res\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return RAAGWord(self.letters + other.letters, self.adj_matrix)\n",
    "\n",
    "    def inv(self):\n",
    "        return RAAGWord([-l for l in reversed(self.letters)], self.adj_matrix)\n",
    "\n",
    "# --- 2. ALGEBRAIC MOVES & FAN TOPOLOGY ---\n",
    "def apply_nielsen_moves_fixed(subgroup_basis, num_moves=10):\n",
    "    scrambled = list(subgroup_basis)\n",
    "    rank = len(scrambled)\n",
    "    for _ in range(num_moves):\n",
    "        i, j = random.sample(range(rank), 2)\n",
    "        op = random.choice(['mul', 'inv_mul'])\n",
    "        if op == 'mul':\n",
    "            scrambled[i] = (scrambled[i][0] * scrambled[j][0], scrambled[i][1] * scrambled[j][1])\n",
    "        else:\n",
    "            j_inv = (scrambled[j][0].inv(), scrambled[j][1].inv())\n",
    "            scrambled[i] = (scrambled[i][0] * j_inv[0], scrambled[i][1] * j_inv[1])\n",
    "    return scrambled\n",
    "\n",
    "def subgroup_to_graph(subgroup):\n",
    "    \"\"\"FAN TOPOLOGY: Connects every letter directly to its hub.\"\"\"\n",
    "    x, edge_index = [], []\n",
    "    for i in range(5): x.append([float(501 + i)]) # 5 Hubs\n",
    "    \n",
    "    curr_idx = 5\n",
    "    for i, (w_a, w_b) in enumerate(subgroup):\n",
    "        hub_idx = i\n",
    "        full_word = w_a.letters + w_b.letters\n",
    "        if not full_word: full_word = [0]\n",
    "        for letter in full_word:\n",
    "            x.append([float(letter)])\n",
    "            edge_index.append([curr_idx, hub_idx])\n",
    "            edge_index.append([hub_idx, curr_idx])\n",
    "            curr_idx += 1\n",
    "            \n",
    "    return Data(x=torch.tensor(x, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous())\n",
    "\n",
    "\n",
    "# --- 4. KERNEL-SAFE DATA GENERATION ---\n",
    "def triplet_generator(batch_size, nielsen_moves):\n",
    "    \"\"\"LAZY GENERATOR: Never creates more than one batch at a time.\"\"\"\n",
    "    while True:\n",
    "        triplets = []\n",
    "        for _ in range(batch_size):\n",
    "            shift = random.randint(-400, 400)\n",
    "            base_sub = [(RAAGWord([shift + i]), RAAGWord([shift + i + 100])) for i in range(5)]\n",
    "            \n",
    "            anchor = apply_nielsen_moves_fixed(base_sub, num_moves=random.randint(1, 5))\n",
    "            pos = apply_nielsen_moves_fixed(base_sub, num_moves=nielsen_moves)\n",
    "            \n",
    "            # Surgical Negative: Change one generator only\n",
    "            neg = copy.deepcopy(anchor)\n",
    "            idx = random.randint(0, 4)\n",
    "            neg[idx] = (RAAGWord([random.randint(-1000, 1000)]), neg[idx][1])\n",
    "            \n",
    "            triplets.append((subgroup_to_graph(anchor), subgroup_to_graph(pos), subgroup_to_graph(neg)))\n",
    "        \n",
    "        yield (Batch.from_data_list([t[0] for t in triplets]),\n",
    "               Batch.from_data_list([t[1] for t in triplets]),\n",
    "               Batch.from_data_list([t[2] for t in triplets]))\n",
    "\n",
    "def generate_triplets_hardened_v2(num_samples, nielsen_moves, surgical_prob=0.5):\n",
    "    \"\"\"\n",
    "    Generates a large batch of triplets.\n",
    "    surgical_prob: Probability of a 'Surgical' negative (hard) vs 'Alphabet' (easy).\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    for _ in range(num_samples):\n",
    "        # 1. Setup Base\n",
    "        shift = random.randint(-600, 600)\n",
    "        # Generators A and B are 100 apart in the alphabet\n",
    "        base_sub = [(RAAGWord([shift + i]), RAAGWord([shift + i + 100])) for i in range(5)]\n",
    "\n",
    "        # 2. Positive Pair (Anchor and Scrambled)\n",
    "        # Anchor gets a light scramble (1-5 moves) so it's not \"perfect\"\n",
    "        anchor_sub = apply_nielsen_moves_fixed(base_sub, num_moves=random.randint(1, 5))\n",
    "        # Positive gets the full scramble\n",
    "        pos_sub = apply_nielsen_moves_fixed(base_sub, num_moves=nielsen_moves)\n",
    "\n",
    "        # 3. Negative Pair\n",
    "        if random.random() < surgical_prob:\n",
    "            # HARD NEGATIVE: Take the anchor and change one generator to a random letter\n",
    "            neg_sub = copy.deepcopy(anchor_sub)\n",
    "            idx = random.randint(0, 4)\n",
    "            # Replace one generator in the pair with a random one\n",
    "            neg_sub[idx] = (RAAGWord([random.randint(-1000, 1000)]), neg_sub[idx][1])\n",
    "        else:\n",
    "            # EASY NEGATIVE: Pick a completely different alphabet range\n",
    "            # Ensure the new shift is far away from the original shift\n",
    "            alt_shift = random.choice([s for s in range(-600, 600) if abs(s - shift) > 200])\n",
    "            neg_base = [(RAAGWord([alt_shift + i]), RAAGWord([alt_shift + i + 100])) for i in range(5)]\n",
    "            neg_sub = apply_nielsen_moves_fixed(neg_base, num_moves=random.randint(1, 5))\n",
    "\n",
    "        triplets.append((\n",
    "            subgroup_to_graph(anchor_sub),\n",
    "            subgroup_to_graph(pos_sub),\n",
    "            subgroup_to_graph(neg_sub)\n",
    "        ))\n",
    "    return triplets\n",
    "\n",
    "# --- REFINED MODEL FOR NUMERICAL REASONING ---\n",
    "class UniversalHubGNN(nn.Module):\n",
    "    def __init__(self, hidden=256): # Increased hidden size for 512 batch\n",
    "        super().__init__()\n",
    "        # Linear encoder treats node IDs as coordinates, not just labels\n",
    "        self.node_encoder = nn.Linear(1, hidden) \n",
    "        self.lin_deg = nn.Linear(1, hidden)\n",
    "        self.convs = nn.ModuleList([\n",
    "            GINConv(nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, hidden)))\n",
    "            for _ in range(4) # Reduced layers to 4 to speed up 512-batch processing\n",
    "        ])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden) for _ in range(4)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(5 * hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 64)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, data):\n",
    "        # Normalize node values to help the Linear layer see the 'Alphabet'\n",
    "        x = self.node_encoder(data.x / 1000.0) \n",
    "        \n",
    "        row, _ = data.edge_index\n",
    "        deg = torch.zeros((data.x.size(0), 1), device=data.x.device)\n",
    "        deg.scatter_add_(0, row.unsqueeze(1), torch.ones((row.size(0), 1), device=data.x.device))\n",
    "        x = x + self.lin_deg(deg)\n",
    "\n",
    "        for conv, ln in zip(self.convs, self.lns):\n",
    "            h = conv(x, data.edge_index)\n",
    "            x = ln(F.relu(h) + x) # Removed Dropout to stabilize large batch loss\n",
    "\n",
    "        # Pooling logic...\n",
    "        num_graphs = data.num_graphs if hasattr(data, 'num_graphs') else 1\n",
    "        hub_embeddings = []\n",
    "        if hasattr(data, 'ptr') and data.ptr is not None:\n",
    "            for i in range(num_graphs):\n",
    "                start = data.ptr[i]\n",
    "                hub_embeddings.append(x[start : start + 5].reshape(-1))\n",
    "        else:\n",
    "            hub_embeddings.append(x[:5].reshape(-1))\n",
    "\n",
    "        return F.normalize(self.fc(torch.stack(hub_embeddings)), p=2, dim=1)\n",
    "\n",
    "# --- SPEED-OPTIMIZED TRAINING ---\n",
    "device = torch.device('cuda')\n",
    "model = UniversalHubGNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005) # Higher LR for large batch\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "\n",
    "# Generate a large buffer ONCE to keep GPU saturated\n",
    "print(\"Generating Initial Buffer...\")\n",
    "train_buffer = generate_triplets_hardened_v2(2048, nielsen_moves=15)\n",
    "loader = DataLoader(train_buffer, batch_size=128, shuffle=True) \n",
    "# Note: 128 is a safer sweet spot for 6GB than 512.\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for a, p, n in loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Fast transfer to GPU\n",
    "        ea = model.forward_one(a.to(device))\n",
    "        ep = model.forward_one(p.to(device))\n",
    "        en = model.forward_one(n.to(device))\n",
    "        \n",
    "        loss = criterion(ea, ep, en)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Refresh 10% of the buffer every epoch to keep it \"new\" without a full stop\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {total_loss/len(loader):.4f}\")\n",
    "        # Partial refresh logic can go here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6458f5b3-17d1-4c44-8052-8f6de0d578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1/10\n",
      "üîì Hiding secret in Vault Slot: 27...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9979\n",
      "Correct Index: 27 | Predicted Index: 27\n",
      "‚úÖ SUCCESS: The GNN cracked the vault!\n",
      "\n",
      "Trial 2/10\n",
      "üîì Hiding secret in Vault Slot: 4...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9885\n",
      "Correct Index: 4 | Predicted Index: 49\n",
      "‚ùå FAILURE: The secret was ranked #4 out of 100\n",
      "\n",
      "Trial 3/10\n",
      "üîì Hiding secret in Vault Slot: 71...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9592\n",
      "Correct Index: 71 | Predicted Index: 11\n",
      "‚ùå FAILURE: The secret was ranked #2 out of 100\n",
      "\n",
      "Trial 4/10\n",
      "üîì Hiding secret in Vault Slot: 48...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9973\n",
      "Correct Index: 48 | Predicted Index: 48\n",
      "‚úÖ SUCCESS: The GNN cracked the vault!\n",
      "\n",
      "Trial 5/10\n",
      "üîì Hiding secret in Vault Slot: 66...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9822\n",
      "Correct Index: 66 | Predicted Index: 66\n",
      "‚úÖ SUCCESS: The GNN cracked the vault!\n",
      "\n",
      "Trial 6/10\n",
      "üîì Hiding secret in Vault Slot: 33...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9549\n",
      "Correct Index: 33 | Predicted Index: 33\n",
      "‚úÖ SUCCESS: The GNN cracked the vault!\n",
      "\n",
      "Trial 7/10\n",
      "üîì Hiding secret in Vault Slot: 26...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9903\n",
      "Correct Index: 26 | Predicted Index: 26\n",
      "‚úÖ SUCCESS: The GNN cracked the vault!\n",
      "\n",
      "Trial 8/10\n",
      "üîì Hiding secret in Vault Slot: 48...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9933\n",
      "Correct Index: 48 | Predicted Index: 71\n",
      "‚ùå FAILURE: The secret was ranked #2 out of 100\n",
      "\n",
      "Trial 9/10\n",
      "üîì Hiding secret in Vault Slot: 92...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.8868\n",
      "Correct Index: 92 | Predicted Index: 97\n",
      "‚ùå FAILURE: The secret was ranked #2 out of 100\n",
      "\n",
      "Trial 10/10\n",
      "üîì Hiding secret in Vault Slot: 79...\n",
      "\n",
      "--- CRACK RESULT ---\n",
      "Top Similarity: 0.9612\n",
      "Correct Index: 79 | Predicted Index: 70\n",
      "‚ùå FAILURE: The secret was ranked #4 out of 100\n",
      "\n",
      "üèÜ Final Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "def test_vault_crack(model, device, num_decoys=99, nielsen_moves=20):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Create the 'Secret' (The Anchor)\n",
    "    secret_shift = random.randint(-400, 400)\n",
    "    secret_basis = [(RAAGWord([secret_shift + i]), RAAGWord([secret_shift + i + 100])) for i in range(5)]\n",
    "    anchor_graph = subgroup_to_graph(secret_basis).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the 'fingerprint' of our secret\n",
    "        anchor_emb = model.forward_one(anchor_graph)\n",
    "\n",
    "    # 2. Setup the Vault\n",
    "    vault_graphs = []\n",
    "    correct_idx = random.randint(0, num_decoys)\n",
    "    \n",
    "    print(f\"üîì Hiding secret in Vault Slot: {correct_idx}...\")\n",
    "\n",
    "    for i in range(num_decoys + 1):\n",
    "        if i == correct_idx:\n",
    "            # The True Secret (heavily scrambled)\n",
    "            sub = apply_nielsen_moves_fixed(secret_basis, num_moves=nielsen_moves)\n",
    "        else:\n",
    "            # Decoy: Different alphabet, different algebra\n",
    "            r_shift = random.choice([s for s in range(-600, 600) if abs(s - secret_shift) > 150])\n",
    "            sub = [(RAAGWord([r_shift + j]), RAAGWord([r_shift + j + 100])) for j in range(5)]\n",
    "            sub = apply_nielsen_moves_fixed(sub, num_moves=random.randint(1, 10))\n",
    "            \n",
    "        vault_graphs.append(subgroup_to_graph(sub))\n",
    "\n",
    "    # 3. The Crack Operation (Batch Processing for Speed)\n",
    "    loader = DataLoader(vault_graphs, batch_size=32)\n",
    "    vault_embs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            emb = model.forward_one(batch.to(device))\n",
    "            vault_embs.append(emb)\n",
    "    \n",
    "    vault_embs = torch.cat(vault_embs, dim=0)\n",
    "\n",
    "    # 4. Calculate Similarities\n",
    "    # We use Cosine Similarity to see how close the 'fingerprint' is to each vault item\n",
    "    similarities = F.cosine_similarity(anchor_emb, vault_embs).cpu().numpy()\n",
    "    \n",
    "    # 5. Results\n",
    "    predicted_idx = np.argmax(similarities)\n",
    "    top_5_indices = np.argsort(similarities)[-5:][::-1]\n",
    "    \n",
    "    print(f\"\\n--- CRACK RESULT ---\")\n",
    "    print(f\"Top Similarity: {similarities[predicted_idx]:.4f}\")\n",
    "    print(f\"Correct Index: {correct_idx} | Predicted Index: {predicted_idx}\")\n",
    "    \n",
    "    if predicted_idx == correct_idx:\n",
    "        print(\"‚úÖ SUCCESS: The GNN cracked the vault!\")\n",
    "    else:\n",
    "        rank = list(np.argsort(similarities)[::-1]).index(correct_idx) + 1\n",
    "        print(f\"‚ùå FAILURE: The secret was ranked #{rank} out of {num_decoys+1}\")\n",
    "        \n",
    "    return predicted_idx == correct_idx\n",
    "\n",
    "# Run 10 trials to get an accuracy percentage\n",
    "success_count = 0\n",
    "trials = 10\n",
    "\n",
    "for t in range(trials):\n",
    "    print(f\"\\nTrial {t+1}/{trials}\")\n",
    "    if test_vault_crack(model, device, num_decoys=99, nielsen_moves=20):\n",
    "        success_count += 1\n",
    "\n",
    "print(f\"\\nüèÜ Final Accuracy: {success_count/trials * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098c5ce-5d93-4bb6-89d8-39498edb177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
