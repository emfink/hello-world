{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22f2f91-9c9b-420c-9a01-eebb35507fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start programme...\n",
      "Start traininig...\n",
      "Epoch 000 | Hard Negative Loss: 1.0069\n",
      "Epoch 001 | Hard Negative Loss: 0.8622\n",
      "Epoch 002 | Hard Negative Loss: 0.8322\n",
      "Epoch 003 | Hard Negative Loss: 0.8282\n",
      "Epoch 004 | Hard Negative Loss: 0.8173\n",
      "Epoch 005 | Hard Negative Loss: 0.7901\n",
      "Epoch 006 | Hard Negative Loss: 0.7916\n",
      "Epoch 007 | Hard Negative Loss: 0.7873\n",
      "Epoch 008 | Hard Negative Loss: 0.7661\n",
      "Epoch 009 | Hard Negative Loss: 0.7555\n",
      "Epoch 010 | Hard Negative Loss: 0.7193\n",
      "Epoch 011 | Hard Negative Loss: 0.6846\n",
      "Epoch 012 | Hard Negative Loss: 0.6487\n",
      "Epoch 013 | Hard Negative Loss: 0.6312\n",
      "Epoch 014 | Hard Negative Loss: 0.5881\n",
      "Epoch 015 | Hard Negative Loss: 0.5758\n",
      "Epoch 016 | Hard Negative Loss: 0.5806\n",
      "Epoch 017 | Hard Negative Loss: 0.5910\n",
      "Epoch 018 | Hard Negative Loss: 0.5581\n",
      "Epoch 019 | Hard Negative Loss: 0.5660\n",
      "Epoch 020 | Hard Negative Loss: 0.5634\n",
      "Epoch 021 | Hard Negative Loss: 0.5526\n",
      "Epoch 022 | Hard Negative Loss: 0.5486\n",
      "Epoch 023 | Hard Negative Loss: 0.5805\n",
      "Epoch 024 | Hard Negative Loss: 0.5566\n",
      "Epoch 025 | Hard Negative Loss: 0.5654\n",
      "Epoch 026 | Hard Negative Loss: 0.5399\n",
      "Epoch 027 | Hard Negative Loss: 0.5725\n",
      "Epoch 028 | Hard Negative Loss: 0.5689\n",
      "Epoch 029 | Hard Negative Loss: 0.5599\n",
      "Epoch 030 | Hard Negative Loss: 0.5525\n",
      "Epoch 031 | Hard Negative Loss: 0.5489\n",
      "Epoch 032 | Hard Negative Loss: 0.5923\n",
      "Epoch 033 | Hard Negative Loss: 0.5394\n",
      "Epoch 034 | Hard Negative Loss: 0.5737\n",
      "Epoch 035 | Hard Negative Loss: 0.5528\n",
      "Epoch 036 | Hard Negative Loss: 0.5478\n",
      "Epoch 037 | Hard Negative Loss: 0.5401\n",
      "Epoch 038 | Hard Negative Loss: 0.5249\n",
      "Epoch 039 | Hard Negative Loss: 0.5526\n",
      "Epoch 040 | Hard Negative Loss: 0.5413\n",
      "Epoch 041 | Hard Negative Loss: 0.5093\n",
      "Epoch 042 | Hard Negative Loss: 0.5293\n",
      "Epoch 043 | Hard Negative Loss: 0.5335\n",
      "Epoch 044 | Hard Negative Loss: 0.5182\n",
      "Epoch 045 | Hard Negative Loss: 0.5352\n",
      "Epoch 046 | Hard Negative Loss: 0.4940\n",
      "Epoch 047 | Hard Negative Loss: 0.5468\n",
      "Epoch 048 | Hard Negative Loss: 0.5469\n",
      "Epoch 049 | Hard Negative Loss: 0.5611\n",
      "Epoch 050 | Hard Negative Loss: 0.5285\n",
      "Epoch 051 | Hard Negative Loss: 0.5556\n",
      "Epoch 052 | Hard Negative Loss: 0.5443\n",
      "Epoch 053 | Hard Negative Loss: 0.4659\n",
      "Epoch 054 | Hard Negative Loss: 0.5190\n",
      "Epoch 055 | Hard Negative Loss: 0.4453\n",
      "Epoch 056 | Hard Negative Loss: 0.3697\n",
      "Epoch 057 | Hard Negative Loss: 0.3433\n",
      "Epoch 058 | Hard Negative Loss: 0.3295\n",
      "Epoch 059 | Hard Negative Loss: 0.3713\n",
      "Epoch 060 | Hard Negative Loss: 0.3487\n",
      "Epoch 061 | Hard Negative Loss: 0.3462\n",
      "Epoch 062 | Hard Negative Loss: 0.3191\n",
      "Epoch 063 | Hard Negative Loss: 0.2817\n",
      "Epoch 064 | Hard Negative Loss: 0.3150\n",
      "Epoch 065 | Hard Negative Loss: 0.2804\n",
      "Epoch 066 | Hard Negative Loss: 0.2901\n",
      "Epoch 067 | Hard Negative Loss: 0.3034\n",
      "Epoch 068 | Hard Negative Loss: 0.3116\n",
      "Epoch 069 | Hard Negative Loss: 0.3327\n",
      "Epoch 070 | Hard Negative Loss: 0.2732\n",
      "Epoch 071 | Hard Negative Loss: 0.3006\n",
      "Epoch 072 | Hard Negative Loss: 0.2748\n",
      "Epoch 073 | Hard Negative Loss: 0.3068\n",
      "Epoch 074 | Hard Negative Loss: 0.3207\n",
      "Epoch 075 | Hard Negative Loss: 0.3033\n",
      "Epoch 076 | Hard Negative Loss: 0.2949\n",
      "Epoch 077 | Hard Negative Loss: 0.3132\n",
      "Epoch 078 | Hard Negative Loss: 0.2817\n",
      "Epoch 079 | Hard Negative Loss: 0.2777\n",
      "Epoch 080 | Hard Negative Loss: 0.3033\n",
      "Epoch 081 | Hard Negative Loss: 0.2873\n",
      "Epoch 082 | Hard Negative Loss: 0.2698\n",
      "Epoch 083 | Hard Negative Loss: 0.2789\n",
      "Epoch 084 | Hard Negative Loss: 0.2797\n",
      "Epoch 085 | Hard Negative Loss: 0.3035\n",
      "Epoch 086 | Hard Negative Loss: 0.2950\n",
      "Epoch 087 | Hard Negative Loss: 0.3249\n",
      "Epoch 088 | Hard Negative Loss: 0.2871\n",
      "Epoch 089 | Hard Negative Loss: 0.2853\n",
      "Epoch 090 | Hard Negative Loss: 0.3358\n",
      "Epoch 091 | Hard Negative Loss: 0.2974\n",
      "Epoch 092 | Hard Negative Loss: 0.2759\n",
      "Epoch 093 | Hard Negative Loss: 0.2831\n",
      "Epoch 094 | Hard Negative Loss: 0.2835\n",
      "Epoch 095 | Hard Negative Loss: 0.2732\n",
      "Epoch 096 | Hard Negative Loss: 0.2743\n",
      "Epoch 097 | Hard Negative Loss: 0.2750\n",
      "Epoch 098 | Hard Negative Loss: 0.2995\n",
      "Epoch 099 | Hard Negative Loss: 0.2738\n",
      "Epoch 100 | Hard Negative Loss: 0.2795\n",
      "Epoch 101 | Hard Negative Loss: 0.3086\n",
      "Epoch 102 | Hard Negative Loss: 0.2760\n",
      "Epoch 103 | Hard Negative Loss: 0.2617\n",
      "Epoch 104 | Hard Negative Loss: 0.2562\n",
      "Epoch 105 | Hard Negative Loss: 0.3056\n",
      "Epoch 106 | Hard Negative Loss: 0.2852\n",
      "Epoch 107 | Hard Negative Loss: 0.2929\n",
      "Epoch 108 | Hard Negative Loss: 0.2967\n",
      "Epoch 109 | Hard Negative Loss: 0.3171\n",
      "Epoch 110 | Hard Negative Loss: 0.2878\n",
      "Epoch 111 | Hard Negative Loss: 0.3412\n",
      "Epoch 112 | Hard Negative Loss: 0.2947\n",
      "Epoch 113 | Hard Negative Loss: 0.2714\n",
      "Epoch 114 | Hard Negative Loss: 0.3027\n",
      "Epoch 115 | Hard Negative Loss: 0.2960\n",
      "Epoch 116 | Hard Negative Loss: 0.2857\n",
      "Epoch 117 | Hard Negative Loss: 0.2892\n",
      "Epoch 118 | Hard Negative Loss: 0.2715\n",
      "Epoch 119 | Hard Negative Loss: 0.2671\n",
      "Epoch 120 | Hard Negative Loss: 0.2741\n",
      "Epoch 121 | Hard Negative Loss: 0.2781\n",
      "Epoch 122 | Hard Negative Loss: 0.2825\n",
      "Epoch 123 | Hard Negative Loss: 0.2804\n",
      "Epoch 124 | Hard Negative Loss: 0.2531\n",
      "Epoch 125 | Hard Negative Loss: 0.3052\n",
      "Epoch 126 | Hard Negative Loss: 0.2843\n",
      "Epoch 127 | Hard Negative Loss: 0.2744\n",
      "Epoch 128 | Hard Negative Loss: 0.2912\n",
      "Epoch 129 | Hard Negative Loss: 0.2816\n",
      "Epoch 130 | Hard Negative Loss: 0.2557\n",
      "Epoch 131 | Hard Negative Loss: 0.2838\n",
      "Epoch 132 | Hard Negative Loss: 0.2710\n",
      "Epoch 133 | Hard Negative Loss: 0.2931\n",
      "Epoch 134 | Hard Negative Loss: 0.2892\n",
      "Epoch 135 | Hard Negative Loss: 0.2732\n",
      "Epoch 136 | Hard Negative Loss: 0.2943\n",
      "Epoch 137 | Hard Negative Loss: 0.2793\n",
      "Epoch 138 | Hard Negative Loss: 0.2963\n",
      "Epoch 139 | Hard Negative Loss: 0.2919\n",
      "Epoch 140 | Hard Negative Loss: 0.2890\n",
      "Epoch 141 | Hard Negative Loss: 0.3071\n",
      "Epoch 142 | Hard Negative Loss: 0.2362\n",
      "Epoch 143 | Hard Negative Loss: 0.2624\n",
      "Epoch 144 | Hard Negative Loss: 0.2875\n",
      "Epoch 145 | Hard Negative Loss: 0.2649\n",
      "Epoch 146 | Hard Negative Loss: 0.3271\n",
      "Epoch 147 | Hard Negative Loss: 0.2839\n",
      "Epoch 148 | Hard Negative Loss: 0.2780\n",
      "Epoch 149 | Hard Negative Loss: 0.2650\n",
      "Epoch 150 | Hard Negative Loss: 0.2531\n",
      "Epoch 151 | Hard Negative Loss: 0.2524\n",
      "Epoch 152 | Hard Negative Loss: 0.2573\n",
      "Epoch 153 | Hard Negative Loss: 0.2910\n",
      "Epoch 154 | Hard Negative Loss: 0.2726\n",
      "Epoch 155 | Hard Negative Loss: 0.2869\n",
      "Epoch 156 | Hard Negative Loss: 0.2663\n",
      "Epoch 157 | Hard Negative Loss: 0.2745\n",
      "Epoch 158 | Hard Negative Loss: 0.2571\n",
      "Epoch 159 | Hard Negative Loss: 0.2787\n",
      "Epoch 160 | Hard Negative Loss: 0.2750\n",
      "Epoch 161 | Hard Negative Loss: 0.2714\n",
      "Epoch 162 | Hard Negative Loss: 0.2767\n",
      "Epoch 163 | Hard Negative Loss: 0.2855\n",
      "Epoch 164 | Hard Negative Loss: 0.2639\n",
      "Epoch 165 | Hard Negative Loss: 0.3290\n",
      "Epoch 166 | Hard Negative Loss: 0.2587\n",
      "Epoch 167 | Hard Negative Loss: 0.2606\n",
      "Epoch 168 | Hard Negative Loss: 0.2825\n",
      "Epoch 169 | Hard Negative Loss: 0.3060\n",
      "Epoch 170 | Hard Negative Loss: 0.2569\n",
      "Epoch 171 | Hard Negative Loss: 0.2677\n",
      "Epoch 172 | Hard Negative Loss: 0.2917\n",
      "Epoch 173 | Hard Negative Loss: 0.2458\n",
      "Epoch 174 | Hard Negative Loss: 0.2643\n",
      "Epoch 175 | Hard Negative Loss: 0.2510\n",
      "Epoch 176 | Hard Negative Loss: 0.2453\n",
      "Epoch 177 | Hard Negative Loss: 0.2730\n",
      "Epoch 178 | Hard Negative Loss: 0.2613\n",
      "Epoch 179 | Hard Negative Loss: 0.2688\n",
      "Epoch 180 | Hard Negative Loss: 0.2801\n",
      "Epoch 181 | Hard Negative Loss: 0.2841\n",
      "Epoch 182 | Hard Negative Loss: 0.2666\n",
      "Epoch 183 | Hard Negative Loss: 0.2599\n",
      "Epoch 184 | Hard Negative Loss: 0.2828\n",
      "Epoch 185 | Hard Negative Loss: 0.2553\n",
      "Epoch 186 | Hard Negative Loss: 0.2250\n",
      "Epoch 187 | Hard Negative Loss: 0.2481\n",
      "Epoch 188 | Hard Negative Loss: 0.2813\n",
      "Epoch 189 | Hard Negative Loss: 0.2632\n",
      "Epoch 190 | Hard Negative Loss: 0.2725\n",
      "Epoch 191 | Hard Negative Loss: 0.2924\n",
      "Epoch 192 | Hard Negative Loss: 0.2662\n",
      "Epoch 193 | Hard Negative Loss: 0.2571\n",
      "Epoch 194 | Hard Negative Loss: 0.2674\n",
      "Epoch 195 | Hard Negative Loss: 0.2745\n",
      "Epoch 196 | Hard Negative Loss: 0.2847\n",
      "Epoch 197 | Hard Negative Loss: 0.2527\n",
      "Epoch 198 | Hard Negative Loss: 0.2352\n",
      "Epoch 199 | Hard Negative Loss: 0.2383\n",
      "Epoch 200 | Hard Negative Loss: 0.2479\n",
      "Epoch 201 | Hard Negative Loss: 0.2721\n",
      "Epoch 202 | Hard Negative Loss: 0.2441\n",
      "Epoch 203 | Hard Negative Loss: 0.2893\n",
      "Epoch 204 | Hard Negative Loss: 0.2682\n",
      "Epoch 205 | Hard Negative Loss: 0.2880\n",
      "Epoch 206 | Hard Negative Loss: 0.2455\n",
      "Epoch 207 | Hard Negative Loss: 0.3162\n",
      "Epoch 208 | Hard Negative Loss: 0.2457\n",
      "Epoch 209 | Hard Negative Loss: 0.2691\n",
      "Epoch 210 | Hard Negative Loss: 0.2373\n",
      "Epoch 211 | Hard Negative Loss: 0.2613\n",
      "Epoch 212 | Hard Negative Loss: 0.2875\n",
      "Epoch 213 | Hard Negative Loss: 0.2650\n",
      "Epoch 214 | Hard Negative Loss: 0.2669\n",
      "Epoch 215 | Hard Negative Loss: 0.2326\n",
      "Epoch 216 | Hard Negative Loss: 0.2521\n",
      "Epoch 217 | Hard Negative Loss: 0.2820\n",
      "Epoch 218 | Hard Negative Loss: 0.2568\n",
      "Epoch 219 | Hard Negative Loss: 0.2651\n",
      "Epoch 220 | Hard Negative Loss: 0.2499\n",
      "Epoch 221 | Hard Negative Loss: 0.2587\n",
      "Epoch 222 | Hard Negative Loss: 0.2786\n",
      "Epoch 223 | Hard Negative Loss: 0.2970\n",
      "Epoch 224 | Hard Negative Loss: 0.2631\n",
      "Epoch 225 | Hard Negative Loss: 0.2456\n",
      "Epoch 226 | Hard Negative Loss: 0.2639\n",
      "Epoch 227 | Hard Negative Loss: 0.2700\n",
      "Epoch 228 | Hard Negative Loss: 0.2594\n",
      "Epoch 229 | Hard Negative Loss: 0.2319\n",
      "Epoch 230 | Hard Negative Loss: 0.2581\n",
      "Epoch 231 | Hard Negative Loss: 0.2683\n",
      "Epoch 232 | Hard Negative Loss: 0.2640\n",
      "Epoch 233 | Hard Negative Loss: 0.2470\n",
      "Epoch 234 | Hard Negative Loss: 0.2428\n",
      "Epoch 235 | Hard Negative Loss: 0.2401\n",
      "Epoch 236 | Hard Negative Loss: 0.3005\n",
      "Epoch 237 | Hard Negative Loss: 0.2971\n",
      "Epoch 238 | Hard Negative Loss: 0.2342\n",
      "Epoch 239 | Hard Negative Loss: 0.2759\n",
      "Epoch 240 | Hard Negative Loss: 0.2385\n",
      "Epoch 241 | Hard Negative Loss: 0.2589\n",
      "Epoch 242 | Hard Negative Loss: 0.2728\n",
      "Epoch 243 | Hard Negative Loss: 0.2557\n",
      "Epoch 244 | Hard Negative Loss: 0.2617\n",
      "Epoch 245 | Hard Negative Loss: 0.2785\n",
      "Epoch 246 | Hard Negative Loss: 0.2391\n",
      "Epoch 247 | Hard Negative Loss: 0.2795\n",
      "Epoch 248 | Hard Negative Loss: 0.2479\n",
      "Epoch 249 | Hard Negative Loss: 0.2616\n",
      "Epoch 250 | Hard Negative Loss: 0.2847\n",
      "Epoch 251 | Hard Negative Loss: 0.2235\n",
      "Epoch 252 | Hard Negative Loss: 0.2437\n",
      "Epoch 253 | Hard Negative Loss: 0.2459\n",
      "Epoch 254 | Hard Negative Loss: 0.2446\n",
      "Epoch 255 | Hard Negative Loss: 0.2431\n",
      "Epoch 256 | Hard Negative Loss: 0.2717\n",
      "Epoch 257 | Hard Negative Loss: 0.2627\n",
      "Epoch 258 | Hard Negative Loss: 0.2597\n",
      "Epoch 259 | Hard Negative Loss: 0.2472\n",
      "Epoch 260 | Hard Negative Loss: 0.2906\n",
      "Epoch 261 | Hard Negative Loss: 0.2554\n",
      "Epoch 262 | Hard Negative Loss: 0.2146\n",
      "Epoch 263 | Hard Negative Loss: 0.2703\n",
      "Epoch 264 | Hard Negative Loss: 0.2644\n",
      "Epoch 265 | Hard Negative Loss: 0.2453\n",
      "Epoch 266 | Hard Negative Loss: 0.2332\n",
      "Epoch 267 | Hard Negative Loss: 0.2468\n",
      "Epoch 268 | Hard Negative Loss: 0.2219\n",
      "Epoch 269 | Hard Negative Loss: 0.2298\n",
      "Epoch 270 | Hard Negative Loss: 0.2558\n",
      "Epoch 271 | Hard Negative Loss: 0.2825\n",
      "Epoch 272 | Hard Negative Loss: 0.2732\n",
      "Epoch 273 | Hard Negative Loss: 0.2442\n",
      "Epoch 274 | Hard Negative Loss: 0.3098\n",
      "Epoch 275 | Hard Negative Loss: 0.2224\n",
      "Epoch 276 | Hard Negative Loss: 0.2295\n",
      "Epoch 277 | Hard Negative Loss: 0.2248\n",
      "Epoch 278 | Hard Negative Loss: 0.2392\n",
      "Epoch 279 | Hard Negative Loss: 0.2287\n",
      "Epoch 280 | Hard Negative Loss: 0.2270\n",
      "Epoch 281 | Hard Negative Loss: 0.2510\n",
      "Epoch 282 | Hard Negative Loss: 0.2230\n",
      "Epoch 283 | Hard Negative Loss: 0.2404\n",
      "Epoch 284 | Hard Negative Loss: 0.2728\n",
      "Epoch 285 | Hard Negative Loss: 0.2322\n",
      "Epoch 286 | Hard Negative Loss: 0.2336\n",
      "Epoch 287 | Hard Negative Loss: 0.2375\n",
      "Epoch 288 | Hard Negative Loss: 0.1904\n",
      "Epoch 289 | Hard Negative Loss: 0.2396\n",
      "Epoch 290 | Hard Negative Loss: 0.2152\n",
      "Epoch 291 | Hard Negative Loss: 0.2454\n",
      "Epoch 292 | Hard Negative Loss: 0.2446\n",
      "Epoch 293 | Hard Negative Loss: 0.2426\n",
      "Epoch 294 | Hard Negative Loss: 0.2445\n",
      "Epoch 295 | Hard Negative Loss: 0.2432\n",
      "Epoch 296 | Hard Negative Loss: 0.2297\n",
      "Epoch 297 | Hard Negative Loss: 0.2164\n",
      "Epoch 298 | Hard Negative Loss: 0.2402\n",
      "Epoch 299 | Hard Negative Loss: 0.2406\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv\n",
    "import copy\n",
    "\n",
    "# --- 1. RAAG DEFINITION & REDUCTION ---\n",
    "# Commutation Graph: Even nodes commute with the next odd node\n",
    "RAAG_ADJ = torch.zeros((2000, 2000))\n",
    "\n",
    "for i in range(1999):\n",
    "    if i % 2 == 0:\n",
    "        RAAG_ADJ[i][i+1] = 1\n",
    "        RAAG_ADJ[i+1][i] = 1\n",
    "\n",
    "class RAAGWord:\n",
    "    def __init__(self, letters, adj_matrix=RAAG_ADJ):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.letters = self.reduce(letters)\n",
    "\n",
    "    def reduce(self, lts):\n",
    "        res = []\n",
    "        for l in lts:\n",
    "            if not res:\n",
    "                res.append(l)\n",
    "                continue\n",
    "\n",
    "            # RAAG Reduction: Shuffle past commuting elements to find inverse\n",
    "            idx = len(res) - 1\n",
    "            can_cancel = False\n",
    "            while idx >= 0:\n",
    "                if res[idx] == -l:\n",
    "                    can_cancel = True\n",
    "                    break\n",
    "                # If 'l' doesn't commute with res[idx], it's blocked from moving further\n",
    "                if not self.adj_matrix[abs(l)][abs(res[idx])]:\n",
    "                    break\n",
    "                idx -= 1\n",
    "\n",
    "            if can_cancel:\n",
    "                res.pop(idx)\n",
    "            else:\n",
    "                res.append(l)\n",
    "        return res\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return RAAGWord(self.letters + other.letters, self.adj_matrix)\n",
    "\n",
    "    def inv(self):\n",
    "        return RAAGWord([-l for l in reversed(self.letters)], self.adj_matrix)\n",
    "\n",
    "# --- 2. ALGEBRAIC MOVES & TOPOLOGY ---\n",
    "def nielsen_move(gens):\n",
    "    new_gens = list(gens)\n",
    "    i, j = random.sample(range(len(gens)), 2)\n",
    "    op = random.choice(['mul', 'inv_mul'])\n",
    "    if op == 'mul':\n",
    "        new_gens[i] = (new_gens[i][0] * new_gens[j][0], new_gens[i][1] * new_gens[j][1])\n",
    "    else:\n",
    "        j_inv = (new_gens[j][0].inv(), new_gens[j][1].inv())\n",
    "        new_gens[i] = (new_gens[i][0] * j_inv[0], new_gens[i][1] * j_inv[1])\n",
    "    return new_gens\n",
    "\n",
    "def subgroup_to_graph(subgroup):\n",
    "    x, edge_index = [], []\n",
    "    for i in range(5): x.append([float(501 + i)]) # 5 Hubs\n",
    "    curr_idx = 5\n",
    "    for i, (w_a, w_b) in enumerate(subgroup):\n",
    "        hub_idx = i\n",
    "        full_word = w_a.letters + w_b.letters\n",
    "        if not full_word: full_word = [0]\n",
    "        \n",
    "        word_start_idx = curr_idx\n",
    "        for j, letter in enumerate(full_word):\n",
    "            x.append([float(letter)])\n",
    "            edge_index.append([curr_idx, hub_idx]); edge_index.append([hub_idx, curr_idx])\n",
    "            if j > 0:\n",
    "                edge_index.append([curr_idx - 1, curr_idx]); edge_index.append([curr_idx, curr_idx - 1])\n",
    "            curr_idx += 1\n",
    "        \n",
    "        # KEY FIX: Connect the end of the word back to the hub (Cycle)\n",
    "        # This keeps the receptive field small even for long words\n",
    "        word_end_idx = curr_idx - 1\n",
    "        edge_index.append([word_end_idx, hub_idx]); edge_index.append([hub_idx, word_end_idx])\n",
    "        \n",
    "    return Data(x=torch.tensor(x, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous())\n",
    "\n",
    "    \n",
    "# --- 3. MODEL: UNIVERSAL HUB GNN ---\n",
    "class UniversalHubGNN(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.embed = nn.Embedding(2001, hidden) # Handle range -1000 to 1000\n",
    "        self.lin_deg = nn.Linear(1, hidden)\n",
    "        self.convs = nn.ModuleList([\n",
    "            GINConv(nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, hidden)))\n",
    "            for _ in range(6)\n",
    "        ])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden) for _ in range(6)])\n",
    "        # Input to FC is 5 hubs * hidden_size\n",
    "        self.fc = nn.Linear(5 * hidden, 64)\n",
    "\n",
    "        # Add this method to your UniversalHubGNN class\n",
    "        def forward(self, x, edge_index, batch=None, **kwargs):\n",
    "            from torch_geometric.data import Data\n",
    "            # Reconstruct the object your forward_one expects\n",
    "            data = Data(x=x, edge_index=edge_index, batch=batch)\n",
    "            return self.forward_one(data)\n",
    "\n",
    "    def forward_one(self, data):\n",
    "        # Normalizing input indices for embedding\n",
    "        idx = torch.clamp((data.x.view(-1) + 1000).long(), 0, 2000)\n",
    "        x = self.embed(idx)\n",
    "\n",
    "        # Degree-based structural features\n",
    "        row, _ = data.edge_index\n",
    "        deg = torch.zeros((data.x.size(0), 1), device=data.x.device)\n",
    "        deg.scatter_add_(0, row.unsqueeze(1), torch.ones((row.size(0), 1), device=data.x.device))\n",
    "        x = x + self.lin_deg(deg)\n",
    "\n",
    "        for conv, ln in zip(self.convs, self.lns):\n",
    "            h = conv(x, data.edge_index)\n",
    "            x = ln(F.dropout(F.relu(h), p=0.1, training=self.training) + x)\n",
    "\n",
    "        # Robust Slot-Aware Pooling\n",
    "        num_graphs = data.num_graphs if hasattr(data, 'num_graphs') else 1\n",
    "        hub_embeddings = []\n",
    "        if hasattr(data, 'ptr') and data.ptr is not None:\n",
    "            for i in range(num_graphs):\n",
    "                start = data.ptr[i]\n",
    "                hub_embeddings.append(x[start : start + 5].reshape(-1))\n",
    "        else:\n",
    "            hub_embeddings.append(x[:5].reshape(-1))\n",
    "\n",
    "        x_final = torch.stack(hub_embeddings)\n",
    "        return F.normalize(self.fc(x_final), p=2, dim=1)\n",
    "\n",
    "def generate_triplets_hardened(num_samples, nielsen_moves):\n",
    "    triplets = []\n",
    "    for _ in range(num_samples):\n",
    "        shift = random.randint(-400, 400)\n",
    "        A = [RAAGWord([shift + i]) for i in range(1, 6)]\n",
    "        B = [RAAGWord([shift + i + 100]) for i in range(1, 6)]\n",
    "        anchor_sub = [(A[i], B[i]) for i in range(5)]\n",
    "\n",
    "        # Positive: Heavily scrambled (Up to 50 moves)\n",
    "        pos_sub = anchor_sub\n",
    "        for _ in range(random.randint(nielsen_moves, nielsen_moves + 20)):\n",
    "            pos_sub = nielsen_move(pos_sub)\n",
    "\n",
    "        # Hard Negative Logic: Pick one of 3 \"Brutal\" cases\n",
    "        neg_type = random.choice([\"Surgical\", \"Shift_Off_By_One\", \"Algebra_Fake\"])\n",
    "        \n",
    "        if neg_type == \"Surgical\":\n",
    "            # Change exactly one letter in one generator\n",
    "            neg_sub = copy.deepcopy(anchor_sub)\n",
    "            idx = random.randint(0, 4)\n",
    "            neg_sub[idx] = (RAAGWord([999]), anchor_sub[idx][1])\n",
    "            \n",
    "        elif neg_type == \"Shift_Off_By_One\":\n",
    "            # The entire basis is shifted by just 1 or 2 units\n",
    "            small_shift = shift + random.choice([-2, -1, 1, 2])\n",
    "            neg_sub = [(RAAGWord([small_shift + i]), RAAGWord([small_shift + i + 100])) for i in range(1, 6)]\n",
    "            \n",
    "        else: # Algebra_Fake\n",
    "            # Use the same letters but change the RAAG structure (Commutator)\n",
    "            neg_sub = list(anchor_sub)\n",
    "            i, j = random.sample(range(5), 2)\n",
    "            neg_sub[random.randint(0, 4)] = (anchor_sub[i][0] * anchor_sub[j][0], anchor_sub[i][1])\n",
    "\n",
    "        triplets.append((\n",
    "            subgroup_to_graph(anchor_sub),\n",
    "            subgroup_to_graph(pos_sub),\n",
    "            subgroup_to_graph(neg_sub)\n",
    "        ))\n",
    "    return triplets\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def collate(batch):\n",
    "    return (Batch.from_data_list([b[0] for b in batch]),\n",
    "            Batch.from_data_list([b[1] for b in batch]),\n",
    "            Batch.from_data_list([b[2] for b in batch]))\n",
    "\n",
    "print('Start programme...')\n",
    "# --- 5. EXECUTION ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neg_cases = [\"Alphabet\", \"Identity\", \"Commutator\", \"Swap\"]\n",
    "\n",
    "model = UniversalHubGNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001) # Slower learning for harder tasks\n",
    "criterion = nn.TripletMarginLoss(margin=1.0) # Increased margin for better separation\n",
    "\n",
    "print('Start traininig...')\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    # Train on 20-40 moves complexity\n",
    "    train_data = generate_triplets_hardened(512, nielsen_moves=20)\n",
    "    loader = DataLoader(train_data, batch_size=16, collate_fn=collate, shuffle=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for a, p, n in loader:\n",
    "        optimizer.zero_grad()\n",
    "        ea, ep, en = model.forward_one(a.to(device)), model.forward_one(p.to(device)), model.forward_one(n.to(device))\n",
    "        loss = criterion(ea, ep, en)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch:03d} | Hard Negative Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "torch.save(model, 'hardened_cracker_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6458f5b3-17d1-4c44-8052-8f6de0d578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”“ [SIMPLIFIED] Hiding secret in Vault Slot: 56...\n",
      "ðŸ”Ž GNN Scanning Simplified Vault...\n",
      "\n",
      "==================================================\n",
      "SIMPLIFIED CRACK RESULT\n",
      "==================================================\n",
      "Highest Similarity: 0.8715 at Index 8\n",
      "Target Secret was at: 56\n",
      "âŒ FAILURE: But the secret was at rank >5\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def execute_simplified_crack(model, device, secret_shift=42):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Create the 'Secret Fingerprint' (The Anchor)\n",
    "    # Using your specific generator pattern: A = x_i, B = x_{i+100}\n",
    "    secret_basis = [(RAAGWord([secret_shift + i]), RAAGWord([secret_shift + i + 100])) for i in range(5)]\n",
    "    anchor_graph = subgroup_to_graph(secret_basis).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        anchor_emb = model.forward_one(anchor_graph)\n",
    "\n",
    "    # 2. Setup the Vault (Simplified)\n",
    "    vault = []\n",
    "    correct_index = random.randint(0, 99)\n",
    "    print(f\"ðŸ”“ [SIMPLIFIED] Hiding secret in Vault Slot: {correct_index}...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        if i == correct_index:\n",
    "            # REDUCED COMPLEXITY: Only 5 moves instead of 40\n",
    "            # This makes the word length much more manageable for the GNN\n",
    "            encrypted = apply_nielsen_moves_fixed(secret_basis, num_moves=5)\n",
    "            vault.append(encrypted)\n",
    "        else:\n",
    "            # GREATER DISTANCE: Ensure decoys are clearly in a different alphabet range\n",
    "            # We pick a random shift that is definitely NOT near our secret 42\n",
    "            r_shift = random.choice([s for s in range(-400, 400) if abs(s - secret_shift) > 150])\n",
    "            \n",
    "            noise_basis = [(RAAGWord([r_shift + j]), RAAGWord([r_shift + j + 100])) for j in range(5)]\n",
    "            \n",
    "            # NO SCRAMBLE for decoys: Makes the background noise \"perfect\"\n",
    "            vault.append(noise_basis)\n",
    "\n",
    "    # 3. The Crack Operation\n",
    "    print(\"ðŸ”Ž GNN Scanning Simplified Vault...\")\n",
    "    similarities = []\n",
    "    \n",
    "    for i, sub in enumerate(vault):\n",
    "        g = subgroup_to_graph(sub).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = model.forward_one(g)\n",
    "            sim = F.cosine_similarity(anchor_emb, emb).item()\n",
    "            similarities.append(sim)\n",
    "            \n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_sim = similarities[best_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nSIMPLIFIED CRACK RESULT\\n{'='*50}\")\n",
    "    print(f\"Highest Similarity: {best_sim:.4f} at Index {best_idx}\")\n",
    "    print(f\"Target Secret was at: {correct_index}\")\n",
    "    \n",
    "    if best_idx == correct_index:\n",
    "        print(\"âœ… SUCCESS: The GNN identified the simplified secret!\")\n",
    "    else:\n",
    "        # Check if the secret was at least in the top 5\n",
    "        top_5_indices = np.argsort(similarities)[-5:][::-1]\n",
    "        print(f\"âŒ FAILURE: But the secret was at rank {list(top_5_indices).index(correct_index) + 1 if correct_index in top_5_indices else '>5'}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Start the easier test\n",
    "execute_simplified_crack(model, device, secret_shift=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0f177c-ba5b-4059-bdd5-0e02e87c19a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”’ Secret hidden at Index: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'apply_nielsen_moves_fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ FAILURE: Secret was Rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39margsort(results)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mindex(correct_index)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mexecute_detailed_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mexecute_detailed_analysis\u001b[0;34m(model, device, secret_shift)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m correct_index:\n\u001b[0;32m---> 19\u001b[0m         vault\u001b[38;5;241m.\u001b[39mappend(\u001b[43mapply_nielsen_moves_fixed\u001b[49m(secret_basis, num_moves\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         r_shift \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(s \u001b[38;5;241m-\u001b[39m secret_shift) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m150\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'apply_nielsen_moves_fixed' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def execute_detailed_analysis(model, device, secret_shift=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Anchor Creation\n",
    "    secret_basis = [(RAAGWord([secret_shift + i]), RAAGWord([secret_shift + i + 100])) for i in range(5)]\n",
    "    anchor_graph = subgroup_to_graph(secret_basis).to(device)\n",
    "    with torch.no_grad():\n",
    "        anchor_emb = model.forward_one(anchor_graph)\n",
    "\n",
    "    # 2. Vault Creation\n",
    "    vault = []\n",
    "    correct_index = random.randint(0, 99)\n",
    "    print(f\"ðŸ”’ Secret hidden at Index: {correct_index}\")\n",
    "\n",
    "    for i in range(100):\n",
    "        if i == correct_index:\n",
    "            vault.append(apply_nielsen_moves_fixed(secret_basis, num_moves=5))\n",
    "        else:\n",
    "            r_shift = random.choice([s for s in range(-400, 400) if abs(s - secret_shift) > 150])\n",
    "            vault.append([(RAAGWord([r_shift + j]), RAAGWord([r_shift + j + 100])) for j in range(5)])\n",
    "\n",
    "    # 3. Scanning and Scoring\n",
    "    similarities = []\n",
    "    for i, sub in enumerate(vault):\n",
    "        g = subgroup_to_graph(sub).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = model.forward_one(g)\n",
    "            sim = F.cosine_similarity(anchor_emb, emb).item()\n",
    "            similarities.append(sim)\n",
    "\n",
    "    # --- 4. DATA VISUALIZATION & ANALYSIS ---\n",
    "    results = np.array(similarities)\n",
    "    top_indices = np.argsort(results)[-10:][::-1] # Get top 10\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{'RANK':<5} | {'INDEX':<7} | {'SIMILARITY':<12} | {'STATUS'}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        status = \"â­ TARGET\" if idx == correct_index else \"\"\n",
    "        print(f\"{rank:<5} | {idx:<7} | {results[idx]:.4f}       | {status}\")\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Plotting the \"Confidence Landscape\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(results, color='gray', alpha=0.5, label='Decoys')\n",
    "    plt.scatter(correct_index, results[correct_index], color='red', s=100, label='Actual Secret', zorder=5)\n",
    "    plt.title(f\"GNN Similarity Landscape (Secret Shift: {secret_shift})\")\n",
    "    plt.xlabel(\"Vault Index\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if top_indices[0] == correct_index:\n",
    "        print(f\"âœ… SUCCESS: Clear separation of {results[correct_index] - np.mean(results):.4f}\")\n",
    "    else:\n",
    "        print(f\"âŒ FAILURE: Secret was Rank {list(np.argsort(results)[::-1]).index(correct_index) + 1}\")\n",
    "\n",
    "execute_detailed_analysis(model, device, secret_shift=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098c5ce-5d93-4bb6-89d8-39498edb177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
